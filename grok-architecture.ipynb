{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8709837,"sourceType":"datasetVersion","datasetId":5224777},{"sourceId":66653,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":55565},{"sourceId":66654,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":55566}],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# <center>From Concept to Execution: Grok Architecture LLM Analysis<center>\n![images (3).png](attachment:52030c4b-0e69-40bc-89f4-b6466e3b5e4b.png)","metadata":{},"attachments":{"52030c4b-0e69-40bc-89f4-b6466e3b5e4b.png":{"image/png":"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"}}},{"cell_type":"markdown","source":"The purpose of this guide is to **`illustrate the specific architecture choices`** implemented in Grok. We will be breaking down the model into its individual components and explaining the reasoning behind each design decision. The Grok model is a large language model that uses a combination of **`Multi-Query Attention (MQA)`** and **`Mixture of Experts (MoE)`** to achieve state-of-the-art performance on a variety of language tasks. The model is trained on a large corpus of text data and is capable of generating coherent and contextually relevant text. In this guide, we will walk through the key components of the Grok model and explain how they work together to achieve superior performance.","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom numpy import sqrt\nfrom typing import *\nimport dataclasses\nimport time as time","metadata":{"ExecuteTime":{"end_time":"2024-06-18T08:15:44.782995Z","start_time":"2024-06-18T08:15:42.944922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Understanding Grok's Architecture\nIn this section of the guide, we will delve into the key components of the Grok architecture and explain how they synergize to achieve superior performance. To facilitate a clear understanding of Grok's architecture, we will use an absurdly small size for tensors and other parameters.\n![1_-lzwq5p6CCnOpPn5WzsScg.png](attachment:e29702a7-aa78-41ea-94cb-836f55ca7576.png)","metadata":{},"attachments":{"e29702a7-aa78-41ea-94cb-836f55ca7576.png":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAgcAAAD6CAMAAAAC7BTnAAADAFBMVEX39/f+8cn788v78c2xyuPB0ufh28mQpbPL3e/L3fHO4PGHpcOPr8f78cv/8c39///ByMK2trbv8vLi4d6kpKT09PL28/bs0JHvzYftzYsLCwkyLyO1rpXk27d1c2HAuqcxMS/JwaE3PEFkclffuFlziln19fP79Mj+88z5/OIsJSKtpowDAgABAwRmkrzs+f7j8vzh7vxtmsjSx6pxa1rz7MocHhaYkXYZFxTn38L++N5oaljXz7DZ1r7Eu56/4oy7345Za0LF6o6x036zzerO3e7J2/DH1eTG0+iYq7ilwN6FrdJ/d2KYk4BHREA3OSyspZSfmH4CAgLf1LQTEQ74/fz568D/8c//8cv98c347sn98c/69dnp15lOTEH/+NP89cv7+NFkX1GFfmvx6cnp372Nomn/9NL97L7++dj/+ND689JEPzIFAwIBAgZdW0ZsbGwBAQD79tKuvsi7wsSascJ5n8S3zeLX4fTO3PL38s7678z/9tT//fLy5sD7+9RXUULp5cjKw6hOTkz+9syIhXDB5o7G6pXJ7pfH7JNKWj8hJSeEhofX5/fb6/Xb7PuSmaGWt9lhYmNhZmmAj59ibXl2eHu94or/79L+8tL/9ND13qz8/fz78M737sz68NKjm4aQiXLCuZokIhrtzor015b+/f7///3///v////98cv58cv78c/rzIf8/NsHBgfJ0snY5/G1xNX+/OfV5PXa6PnV5vXV5/nZ5fnZ5/Xe6vssJx0QDAvi273g3cW93o+mxXnx79f59M2Wsm2dunm94Yt4emuz13+424P59c3I093L2+vA0ePAzduIqsvA0OGDq83U4vEEAgf/9tH++efjw2/nyYNgj7mdnZ3f6fXr6+3Ly8vn5+fV1dWSkImgv9v/9diDnmjQ8ZrH7o7htVrk6ba0v45SVlqmr4rS087V5/arvMrY5PTb29ra6vpdkcT/78/878v97837883988/78Mn/78v/88v88Mf6/Nv+/OX69M/999n+/N//+tf/9M2fZ6x4AAAACXBIWXMAAAsTAAALEwEAmpwYAAAgAElEQVR4nO19C0ATV9q2YUqr9iPU2n5+DXipuy1RkloLIuJl6w3qrooGbJiEiiKWcpnY0FYrgwjeLaKtbS2JCYh+DZKkzmq5qdjPXraXBdZlULet7Zat3//7l1KaGZIY5ZL/nAl3EkhICKTrM7lMkjNJZs4zz3nfM+c975jikUTR6dM7in1VvqfrMydN+q/HHnvsvx+7B4fwX//12KRJ72++qjqtUhWrioZcE2NcWKtDgK/vOtWO+knXrj+fJ6v38kLSkXuwHziO+yDsvAz58evXMp887avwHXJFjCgPThcX79gx6b+P5xcU6HR5eRSto0f60HoUmpubCwr0BpnxdqLppfevHXeiMkeOB6cVinVXrz/GSlOy6AEhxOU0JY+haZSW0TqZSE5RHZ8YY1hs5tloZF5TQjH4SDTw1/0WAY4CxdbFvnS9QAGbhyG0DyPHA8CCay+Y0mUIOtheqilNQIAGFkMQbT6bEnZ+YDKoTVoAk8nEvBbSQq6BxofveI9eAP5TRlp8/Fpm8WkAh2tj5Hjg+/4HzXlKFu7FUg+yjwb8zIw7d8COondm3L1Do1160NLaljRjxp0ZZ9pa4ctEKrk9xcwVDvBVv2lohAVp3PcnPenrq3K4NkaOBy+8VKA10aBRyFMOvHuUV0oqJtkCTnbqZalkZr6oSxDkgTyCD0DyV8LvYNWtwgKF1KD68lsEPDWoGKOOwk3XMn09QQ9Ow2XdjuvHlYh9u6hGTat55Mw0WsOOJKUzcRlXnEcDTRBTyWtIDPKAL1nJlBSulQQmskG7EGMEZMjLwAsoHPHRstmmlPQWsAFNyWiWGJf9pluOGNPx66dPq4odsxFGQA8URb6+9DU8RW+wb8fUiGk1JiCijJrN0RyBXz7OQjQ7d8rllLjOH+Nkrc/KWr/+BFNSvlYQqI/xao29cwel8fadaSlstbw1Ti3Tb9jYLqTZFBUXhybXZZiGsRpGHrjXS9dVitOO+ZDu50FRkaro+PU8tZfRzv1ieMCRFJrN/kQ8Z2Y+JUo5mYptStCoEwOJU1765maNxiIt8rX8NWY85YQf/1RQnT5LEpxEcf2Ji3NMAZuiQ+bqY/CXKqMvRYaeiBm+ShgFyDPonp9UXLTOoVoZAT3wPT3vBXmegdbY2y5AHlRGp+rj4rFAciYinpNKSAgJ56SsPpC42DbjzIyw1jSmJOBBoFyZKiUEfOyENlASrUHat5DzTfWb+HwpeXGemcawUKnUP2D4KmEUIEDWojx+3UEbwf08UKmuaYCHw7V7vxgeRGZVLriEpS7kzMTr/Djz4/I/ipYGxvnzMR6GYTwiiikpX0sGJusXza+jJhN+msRI4oR6gRTLNhQSu/QbT2KnECMhmJI0h/ubbhfU0E5uTrxWsE7la7+NMAI8uPY8sNhQatAd6toxyIPUQuyEv3Tlgkq/OVE8MogSGeOJrPZADBMAELwFYlgStgtyVmJbUkKhwM+gnytYj2ySZCWiqZL1WbvXS3lRNAfzTxQiv2k7kYGJrr92VaEYxTxQ1b9E6xGzA51+aqVpPRa5i581hfRfQMzMP4lJE9Q64xRJajtoF2aE3ZkRdjeRKSlfi61BxEFZAmIm6SfSXKqMBprgn6iL5PMkGMbnTdVxOEF6IXuYDv4oQh6F5l3bUWR/2+BeHvgqTu+4XqDMc2if1DjQg6wEjI9JJwdxZqYv4EkXmnBjsCQV2gcaAB8vGVNSCOyDpEWc0AUJk0k/lp5KJaIk0ZtblH7YgszNACYvgghqNvoMx5EffRBfX6dS2WsmuJcHp1W+155XOmimMe1ClixVSp6iNnP88tOjJav0ic2neKvq/YmZd2bATsY2WJACehCYFE+s3pMWRUbiWnagdD0WT7GF84ldafmtO/doNRwiSPvvwgP8+Avr7O5hdrcebD6O+nAd0wMKMaUSWRkrJWRw3eTKmSJDoVS69tI0zC8B8edLJXzgC2B+TEkhsBPbpxCVJxeESC6K8ig8VIotYstZQfyQXQu3ZC1K9AI80P+78CBA9t9XVfb2MLuVB0WKHdfqaeAsOLQ/XNAuEFsST5JkoHAyx09pTlxJkALs1GaRfA0mAO4CRvKnMSX1QA/qJocK+JJTlacSKCRxi3RmC00Z2tdifJIgCzOagX3wb6MHWqHpmmoU6oGvb9G6a7jjRppRri3MChTOC46fl5gQvHIOJYzLLtxyMg5Ryx+PB9gCbpvg1+oSF2U9rjPP2Rvv3z4lfoFeI1zDCzYzFyNOBsfPnZMh1sXHbx74x+TAkTE0a4dy3PuCMlD2O8fDAmqOSrHDvspxqx6ojr/UPMhgAytARUKDKYZl9GLFUJTah0JpymhW43qaNujxPQa1Ws1CmUuWZhOLomJoJcsA3vYx0K0FIdg8pioQteE2Cj5jmbwGO3QojdZpNGoXAKW1LuHTUAGOk/BasZ0Ngxt5oPK9ek0ptL/foGt/KBQF1q9MZMTpRDFNUXDcEthHcObi4p7+pxKc+oxDKpbjNEUXEIRgU3LHxUkhTZkpUALXDfp71LzN81yBS4lGx0nvSgBOHy8YfTwoVkQ9rza2OL47corCESE3RgQ5gFCAGD4aLtRcymTQwPFZtBfT5BtQxEeoRChUzeLSlCggKzo+MYZi6sLHB5yeCPiYGqwvUW6Omv47l2D+lFg7e86HCeDsKfj96OOBr+q6fPCz0XVorovT212YojqUSo1HPfG7fe/t+8M+Z/E/l6fUi2j5iHZe6t//2D4iuLNdeP64W4cKoY4MSAGFO3jwSdSnn3723h/+5y/OYt+zn31eYNI53hC6ELrEx+y7yuBOHvy+3q3HAKUdGJqkxxGLeAA9+PSLxZ8uee8PThPhvS8vTwkzyIZr/+yBUXN9h12jVt3HA9+rkza69yA4cCYqM/WIpTFXpwMePPu7T9/70kkSvPcsuF2eIh9Rn6He63i2XYLgRj04/vyoHUGKNO+6uCuZq6ZltFof9enSL55dDJqGP7z33nvOEAFs/ezlz9OAZzNiO1bgE3NNZU94ixv14Lpu1F7oQ/EAf4IfObXZi+JqFgIeLP3is8X7vnzWGRaA+1/+8odn3/tdkmnkTEUumvd+2A47OhXdqAcf2ONNoyMBmmbnzTklwHiF85KgfbD0C0iE34FK/MvQBYHZ9g9/eXbJlIyR68hGucKXMu25yOAmHsAhyscHa7BNYXdmnDmTtPOMm3EH4EzbCR5GSIgpiV8t/BTSYOnSy79771lYk+DUfm8odwsT3nt26ZT6gPyRaxKNk3aMHh7AUdSsgEF4UH+RRwr4bge8Ysnn8QhMQEhJgrNyF9QDSAXgPgIjYehtQwee/exlmc8IBttdWzeKeHDa92r9YDzYc0pK8HhwwKH7QZIknxBICYHEz9/SLgBFWPzZvvdcAOA+tlIj141wbYdi8PpxGw9UmfWD+QuJMwlO0OMjhF17CQwjsNQFcyYz7QK8f7n4X585j08/+2zV72QjdulR/N+xo4cHiuKiScK8QQYipc3EKpNo+/uCXQmt+KKAj61+PANJAXaihQZfLN03/V/24cVBPl98ZsRsxcTjx0eP36go8r0m0w0SwMTwQD0y12YyNkk4a+fcZiM0EvUpNBMZIuzbvX//vv0W7GPuPwIc2N8Xf/jxx+6C/XFwyROxohHrRDC9YEcFuYsHp1XXtLRm4D88gnqAXsI+T5LTeAxKpzM8YFqGL95742avCn128SkJGbr+wJKebx5Yuv/HN0IO7D9okwf79z/x8YjFziB5k0ZP/4HidPHv8+3iwcjoATI5zCykfbgIHYNf6vQXln6x7439S5Ys2W9Z9u2/jBGV0dGVHD9YtZ1v7794ef+BxZE3D3S9wyy9XzyxcZB9Hz58Uv/B6LEPfE/7/m+6bpAqHkn7gN0h2xSNivb+ayljIQA92N1Ng/37b4Zw/rUftAqXBf+CL/ct2X8AtAX7IgEPDvzY0QLs37ePoQyQiSUH9lm2PLh/yRN3Rq4vKfaD0XN9geFBnj08GCH7oAtCCk3ae7nDUNz3xkHQuMNzH1boUvJTUKUH9h944433Diy5ufTFFy//uOTmiyFPfHrzs0/37f/sL3958cWlBw8e3H/gsxcvL3lxHyxr2XL/E7GDDYgbPrSONh6MZj3oBEoJacPOLV9+wSgC1IOOFuDgwQMvRv64D7YTS5bsu7nk5nwMk/IXH7gZyudH/rg45+aSxU9wCFL6xM0lf/ATSPmpp/6H2cyiJU/sHDk9GHU88AQ9oIQUhZsuLX6WaRr2vdFRleB+88DvFgMjgPEYQOPwovSz/3NzsWDpkgN+X/x4YPHFm/sXR1/+8eZizhcH3gj58sf9qdIvDxyEHDp4Tw+64TF6AEHp0z56g+EB1IODnXpwcPETQOeXpAoqOcT/PbD4jZs3D3zBf/HgEr/LN28u9jtw840nDhxc8pn08lLs05v7bz4b+iVjLzByMKL2gWfyYKTtAwhKT32191+gaYD2QScRDtx84o0fgSB89q9/LZYu3X/g8uLIaD5h4QHUg5uLPwWn/mXBZ/8SfLn/5v73Qr7c32kl7r+nBx3wKD0Q0VSeOunQi19+8d4bSzpNflCpl7GlUBjeO/giuXT/E/xTT1z+Envx4D6/y0sOLvY7uATwgNGDT/mXgS9xM+TLg93O5T37wALP0gMISl34ry/+sPsmtP+Zuty3/yCfOcfB2U8s/Z/K3wFb8VPyxZsH/ZbuO7D4FNSD/TeXXJZ8tlT6ImDAs5y/LOlqGJbc0wMLPEoPGFCyuENADw7u6+4mXPIvPpG6ePEbfkTql/8Tmvrjj5/yOS/e3B/J2AfAX/gUFPkM++zA4sovfzwQST7bo//5iZR7PIDwOD2gaDom7dZ7bxzoogHsOvjMj8ORcjifHTh44FMC44R+GgpciCckITc79AC0HdLPlhyMJCX8N0L+ss/SewAZ9ERa+ojtimfyYHToAXQf8zSTf3eLcRJ/PAjuBw7A9S+XXl66BL53YN/SL3/88Ysvgel4eenNZ5ceuPklKHHw5uWDP/5488vLX94MWQJL/WhZnki/pwcQHqcHNJy8um7y/M87sPLz/vAj4oOnTAErU15mngA2gXswsXsveA/b1KPo/NjmEdsPz+TB6NADCJQS6U2GmBg2hFzINrPNZrMcPgnBKyPOjseiTGo9WI+JQfVCthAUEbPlOjyVsytv8rSLieADtgXiDHrkxmp7Jg9Gjx4IhRSN6/U4qzeMlieTjkWSUTRtpIxwyFlMd4H8xM17gwuzN942sUwdb+Xh2hG73uihPBg9ekAJUTYwE2ykBvlElkjuzTTFaLVKOPZQgyCajk9Y7AJZQYHQiCjz8c63bo9g6IZn8mD06MHAkMmbsbuGRA+YktMzeTB69GBgGChRm14s9IDkL57JA0/RA5NQJGtNZA2SO2I0wDN54Cl6wMWp+ZKFphGd3cA+eCYPPEUPYjTm1dIok3yk/8fg8EweeIoesER1mCTKcM8+cAi/QT3wMvPXmIwBgyUZG3l4Jg88RQ8oug5rdyCLxMjBM3ngKXrgQ7HDNLTIA1I5eCYPPEUPDDpW9sKCjJHrLrYbnskDT9EDmhLGSxeK7tmJDuG3pwdsAysSixrZiXPtg2fywFP0gC0yY7wFLHuTD44gPJMHnqIHLEMGuYtlvHedySH89vTAKErm3zWN8IzqdsEzeeApemCuE9XXoV4ynGIheoQyUeJ8Ks9o1FEyjRYYkbhGhNfhFB4nkm2gaaFIo6dRRyZ4dSE8kweeogdcPCbqhH/g5jpzS+CaQFGBDvEPDBQJYzID/1+g4TZrcuAaf5GZDlqz5gRuEu8KXHNCNEI08FAeeIoe0LrkeJLk7zLTsaRAuoBiX8JIbB6SNpUgiQRuygmSF5LPZn2OYZieHbebz+NMpuiRmRrHM3ngKXpACc1TyI9mtIiogDt3zqToKTwp7MwnXs0tYVfDxMqUlDux7SzQLJy5c0eso9vv3EmSjxANPJQHnqIHMT51UslCxMas+kqlEu1xBQrsuDZTpCtwz1/rA8/kgafoAW3wkn4+z8u+C01smm45sYsemRgGz+SBp+iByasutEBUb9+/BTsuKwzSJQ7zf7IOz+SBp+iBUKPbUGBi2T1/tvzzBfkj0/nomTzwFD2gEWNccKbdqRWovEv5unv+wm9PD2icdVcaZbKTtTBuVoiOzOBmz+SBp+iBHI/zE0SJ7GwXhBRMLDm8/8gWPJMHnqIHprxkUrqQsi9lOUXJkxY9rxyZXfNMHniKHlDKPeSqeQb7/EZKLgpL3Vxwr1347emBXL5B2qZl25edERUlylIn6zUjsm+eyQNP0QONTqjVmmLsHX+AZqzeLLvXLrhBD1Dkp1defcUl+Clt8OkKcEqUETmPFhtjjCYT1xyD43lKoYwl1JpxFk2na/CeE+Up5WZxNhtmrqfcn7DLM3kwZD1AP3lt63333bcN3pxbvLeauPRgyaRwinUXu99g+uSTtvqkfDwt5c7O5OfBPsSeSTLo8LTWpNgehSlKVGfS0Vy0M5u4O+GZPBi6fYA84L319bFjx24fO26sUxi//cF0o26Qia24dDJGYAky86JoTnQCO+UEwfHLE5s/JzjrTcK4ePJUXI/CyXrcKx/BC4AiDHXvhg7P5MHQ7QOf//jrsq3bl1eNHRu+3JllbFV49QMt3EHyMt/WmUnplHyN+NLcwkIRjSwonLv2eYNwUeHcQBzl/r/Voe09CietXBi7qTk9eERGuXsmD4aqBwjQg4jn7ruvKhwwwaklvKpqvPfW21qZz0BtuYEyS7LrxTiNpLQ0IzRFp6Skp1MxWllBog5YkYV+PXlQcALbVDmXV5iBo+7vW/ZMHgxVDxC6eUWEd83f7tseDqS9Cuo7OLnHDuUZrIV7P2igxAE627/HTUwU1ItoFkzlAq8fGY0xMKcLy2iZNkv2fM+IePEnhEQqCRHjwhHI4+iZPHBCD1ZEVF957u9/rA2vGhve8/R27JlZHzu2GtgI+QONRpZTbQVsSBQK0kCr1dKQAeAZnvK4tldfgZZ7guDz9hpQiu3+CbM8kwcO6QFKo2Ihi2VEQQPNbnkgwvuKd819W8eFV41bzlTr2E5pYB4HfA3sgs51ZiW86k9/vv2xKcZW0kmT0hSWutlkw4qg4pKYnaUymJe42eAnjd/D1ctkWmG+m6dW9UweOKQHKA38cYoxwxFaD/XgSrX3fVtrGRKEj2VOcub8Xh5uqXkbr8d2PHZpAkTt380mrZchz/pPm2TGHdKoGBtdxbpNFxm/EQUeI1AAHyEeJEk4EZwVv3Je8203z6HimTxw0D4QnpyzgYUgKOAB1IPq6m1X/nrf1uXhY7vrt3N1gNeQDmO7FKSjxPazW2Umna2xZGw8mcePom2YktxNoTvhMyUycwEbDDh9JjKEJEkJn3/J3aHynskDx+wDCl3I29SahHfrgfeV6ur7tm4f163+4R1VH277dVV3I9GtB+FV3g9+bCqw0XMsxDdIsAUxtsy+TaFp8Emekd8GdkguNqdlkVIBTyAlohe6uXvZ03iAyk+RlRtxo0OHiW1eTRJzE9p1eIsI+I3brgBUQ0VY/vr25dsZjN9uWQMP48G9drnls/G125eDl6/DAuO3dxRg1sePZx6Xj6uNePC20cZgsgDtHiL2tq3UpMjarFb4LNRlRmbrC2jq9iqJgAGPmNlGM/P6g7uO1iE4cDqHtXvJ03hA6/0klTvTNenpKQ4scYswPkEUikzs/AcY++BK9bZq73MP/s0a/vR3iL+B+5/A8nf4Fly1WhZ+9retT+3J+8QqNrS08FI0zRrrn6anp+PwWWeUx0v33hbRmX58Cw8EfOmijZ9oNAUFKSkaxMcnQNMMaDCcJoOn8QBtnklEf3h1xtW2GWH2L2c+XE0AJqTuStKsqPYGLGCoUP3H8O3b+3cvb68qmfU0g+XLS2dXjds+dtzTZcuXj93eVWJc1wLe3D7+9Wd2hF29a3UJu7pjRhj4tzY+bbvKPF+9GislQ6eEZRMdNBAQ5Pqrd+/OuHMn9k59nFxdkJeHDm+kk6fxgN4zkycBlhQQUIxv9wIgkBA8gkzdBdoFix5cqfFeVl5R3h/nS1+VwC2w6CsXVuecLS1v+Fmy4kJjbneJXLCUMPfyBvDyQjQgmXVI+VIJWKx/iBFSyycCUkKSPAzzl3TyQCoFlCAqQ2ZGRq4PXjU1aHJmflzicF5s9zgeJM+USkg+n7Rx4G1UB4bxSfAgjVz0wPJlwF+AagCcx/LGsoa+KCsvrZH8AuXg8NmKpls542ev4NeUljQ2lHSXKWkoyYV3y6sjv7bHZVjFntjkHURUe8adOzvPfHVmZ1LvT+sLL7bB56SklgUkcXGqT7ZUQAgICw+y4rR4QsLmeZOjghb5z83yq8TI0NRN2e1JSekpFA09UZeaCx7IAyx6x4d3P7zqCNo+XE8CMzx16pk5D/0twhuaidXVV54DPMhtABXa+wZ4wK9qKilpKK2oKCnJuVUjffNCWTl4v7tIbidnmFeHf91oRoViKwtbqMeIKBa35c6dJB9Lj1Z3QTRgZehGuGaU16/3O9m6Ryu6CGRAahGEXVyumuWDIFqdUadLjGtPap8TON9PKo1cuSiz/jbN4rrWavQ4HtTN5DN2onXTywb0uwgeSWzSsFjm26+99ddqxjq4AvWg51negfLzNYKqkoazZ0tLyssuvCadfquxrLYWtAI9tAMwwXIDyAU8sFEpKO1FYlHG/JUYJ0suRFC0Vxuv3BSdxqyINsfjIrVJuWeVVGrhAeGXQdNimlKr1bRQiLLZbEqoFxu0+VGFkVJsfUKyBnFtK+FxPGi+CP1GyjH3OnE9SexNqEOVWlbe7df+xvDgSjXQA1iR/QShtEZw9O233z76zoXyxiMXDuWAIrm55Y2zn+4u1N1AgBeHf03jWs/DoZHJMX/TS3nwQrNRtlckxnt+alozpY5ZyZ+ThFIync7rDDBnMcgDbCFCcdWUjmbTwGWUIUiLskDJQmilLCNjXuB6wczgXe0udR88jgeW/kTHaIBekq6MTdIwJy1rT/sry6DbaNGD3P6CUF5RQ+ZAvNtQfqGkZDr585Hc8trcI8feaYJtA8OcHnLQAHlgo//Ah04mY0VyWisr0M/5ULoZEfa8gsRNFHfoiCXqGWUL2/0kEikpic62Wclq/JM0ffbKi8SpRZko5UWpXZPUyTN5oEYGuNjbD5TwZPYeFm3p3hUiJt0/ajrtA+bU7iMI0E6sLT/SdORIecn42e9yboWOb2xoaDosefPI+fJeamCRgwF4YFCyZZScMquNRtOGDyWTkd6uH9qvtsUJU+Mvpq6aZ6ZsjkZBwa5TZtGC9dKQuRlag9g19qJn8sAhPQDHXklReR0GNiXUiDVPbXuu0z5o6CcIwD7AIsqPlACxqCifQLw2LvTo0yW5P4ccfbOpwaIHveVgAB6A31bTMpEJXnDOvyrdLO/FA3Vd3e2er8H/qzOY69oy6ih6wLk3fZACk3hn1BZs9clWEerIKWETnskDx/SgH5SmV/7kvQ3qgUXjewtCY2kNVvs07Eq6cORrybmzFVU5t0rLS8rffrMpt7ysn3UA/YUWGxOqsxDRjAyUhnEsKMV9HqV6jTTK2xuS5sxe5G/YhREn2mlXRL54Jg8ctA/6wix76ZUHIyKuQB70F4TG8+cIjADgvXlOcmh2bkXjz+Dp9ZJ33zxSUtHhNvaUgwH0gOUll4aJKDg63awTyShhL39BvSk0zvp2dkKoF80nUhMSM5xvGjyTB07qAW7KC3jtrepqwIPcDsOvhytQ0jB71qynZ836Ztbs87MqGnJrmy7Mnl1eUvE28B/Ke3KmcwvbPGAbNkjCtMwwdCUVw+4zHh1fGd3qzG6YcBmtXxRaOTXAeUXwTB44qQdqGcvr9it/62oXegtCRUV5Y2ljyflcUMXnm0rLSktKYSdSBdCDsor+1sFAehDjVUfcNTC1L9JnxGfSwp56IFw01yk9EFNCHGfnZ/FWOd+X4Jk8cFIPGJgm/GlZbe15u1H79puljecbrXxy4devYqz/hs6cnJqBM0NUtel3gd/Ya3gJmpHhgjHqaEahYI7MYHDuWzyTB07qAUSzds/PyxqsVasNlB5950JjqbVPmv4RkG79R1ARlZ6vl5vBKjXnQ+nkPn/bB3F65mU4/2r63NBMkcy5WDjP5IEL9CDPVLDnIYfwZ9ufbLBxNmqEutsIypz08vQZ0s1078iEpKTb1rdzBCiqa7kYmu7kmeGZPHCBHqhxr5QU+wLSGSgpERulfCgrJlkeLrYxnFCEiuLUlBx2F+HK2AV9WgFkr59z/gLNdI7QSm1BZEi+c02MZ/LAUT2AA5ZpZmY6OmZDwFcbOvBV15rjYLG0gyuxOkaDhaE0bBe0uFxEd7uNQM/RDStDU1wUsxK3JTRTq6aG3sx4Jg8c1wPmaIOtkD///MrPLsGrLw3eo2umEskwOZsZn0iJ2pHuSocrdZtCW10U1FqfGb2l3SAcemY4z+SBo3qA4Aj8WnDMm1dsDfceG+48xr41J2XQSa1MBjnWJqIZyeYmRWb23EBI0UH+GsQ1sYwm/VTMv8WJ5tIzeeDoDme2Mx4b4MFXKyKe8e4eej70ZdzyrYPOYONjMOCft3GZi+QsTRgxuRnvJAKCizLy9iTuqZvj2J7YgEmXtiU6Id9n8JI24Jk8cEgPUBqdm3XJhHMpYTONr/ir94M9QlGGjKrl4VvRWJONjoMO1Mm5BTpKCO0DBAnDJrd02ZOyPXu3RCWbEvyiXNIuoBSeIF1ZP/R+Rc/kgaPjD5JPcVbP8fKh9XT6Cu8r294aX7XcWU0YV1VVtTXBRr9BJ7yUMWauUQTtAzXgwWaka8YbFrueqMyaUrmapm3ERjoGszllCpE29O09kweO2gcFhQRBzo/K0AiRB6q8r7z1H+VV4YOf8gPrwb4y6zkAACAASURBVNjlY8O35g/8u2Y5FaaUW6KvhHWTgXXYefazxXsC+XxC0GIS1zm0K9aBikTIJU7g0Ofo9kwesPU61BHcThJIBRLspB7GN16pjnjrLaf1YGz4uKpxY7cO/GeFlB4LE3XGtWl1KNVlFVKyglAehtVrNa4YP8CVC/HE9ZEDt1IDweN4EHeRJxF0DOq1GwSQAzjqT3pxFYx733alZqv3ciaI3RmEL68a/9Yr6SzbESbq23ribocIUFSyuDM2Db5FyfIFUv+dbXfOmE2UjXhph6DOwEw4PcT+CI/jwYaLAg6G8RwDSZJ8ASYgBZzUByLgSKRq763hVeHOSsLY5VXL38qX2XYfKZYX0YZS0E4UUW2VmZ37BkxXFMVb4kOzV0vI6MIEH1dkeBTh8+EPDM0P9Tge1F/kcXYtOnlykUPYtV6KCQS8+ZeSV1Qz4Qs1FiI4aSMARSgBxqJNIpgMXlOTTDrGnZMDfwGx7BwcjgKUoWBBcKVAIJGSlf5pQ6q83qC0/quShzpa0eN4MBR/AUnUYnyCnJnQTresYMJYtlXXVD8T3muamx4t/9hxlq4isBZumRIjfKw18WDi4au25udTJh+r0OAajd5gQMCqiRVGTAayDVYNJzdmyMS4eoO/pdUi+YT/VzJmBDvcCP5jHMFxLa6laPum6LbsZfPkyCGPTPJMHjjqL2iCSTJ00QY9zmbmR2JmRNl2Zeu4quVV/bF9ezgT4A4fw8O3d75lpSTAuOXLwx/4+CUZZRUBAFSiEAWrYj3wGynKCN8u5PhToj10HIewBLbyseh0fbpMLBbLAGglM+LQS6PRdFuVdtQvQp+Rxhr+XdqFIfUf1PND1tTJlJRlfiRLGIv3FW8Y2N4f4L2///1Pf+vzlk2Asv9IjzVaRUFBXpiIsqyzzJk4xWVe3K0ktmTXJZ/ApEycO0byyRQ6T8cgr+NZZ6l8dcclAztq1wutJ4MQ/N/EThxSf2JhYFyGUCln47TXio75D0DbcGXZ7G9mX+i3fPPN050Aa7MhwJq1kmB5+sKF2Q+/O3XXSWCx9LsBI2YqsQA8wjVo0TBvghebJCS25SM/0jLdAbBc+Ccfhwh6PCgoaAFAVFTU5EuXNifkJ8bVAciFdvBAY5RVrkq/Zx/YRmfED4KiQA+W1VRvg4GuV55bVt5Ycr6srLGsrMdjQ0NpTU5oaEhIyLSIptW3Zp99vey5kBVHGs/3KgUfYeHGhtyK2TmVGEkSGIGR4N51IwlSAqwSiQCOfsYEfBI4ryTGlOEBBcC2hAqkTLtAACYw21gAHRwYqA/eIipDQyJ3z997Iijzzp2knWlpYr1Ir8yjhahBg6T4WLSis+bFLGXkljRbFz3g/AnA9NBTiNagx3t1NOBCg00eqFSKYtWo5cEQxyPB687/EQGEgAlzheOVG3Mb+qE8twb7dpbvunWzDuc2TJ9WXn6O99o3Jf0j5Ds3Lbnwq8jLqOtaOm4MDELsrjERvtKkXMUydZaP7kZj6y/FcddLSUswowCTxurrhEK5XK+vY7DTgrC2GVfb4+ZcCjpRGJyV6neqUsoPWT8lcOpkaufGlhaDkUJ6HC2KJQuOjNMMMMOWXJ4w1d9/QSJF9xqAc1uTqG/9QGWlRlSABqpRzIMhXmBFaBRf+OBfLRNhWOLeu0cfd99Ka8iIitcras+ebSiJIA9Vhbw8uzQ3t+dIZeZW1jls+Zt/bLDaDySmjTrTvBaTGSbmySu4ikF/AbxPrcUCDSZWfRBPinU0DFKDSajx8fGC0qWmKANwHZq1iEbzyScaBKGMiE96czPXSyRKyFwQuDI1WkqEblmbUKcH/kj3keCq8wpn7my2fWjQuPkEUJrKLSm3e83YFBC2Ni3Wuh4oiif80k2QUceDIesBqmT99OBfGYeBiWuzIgcN5U010ojXK8obShrPny99R/J2TnlJWUnPaQ869MDyuqThm183BiBW/cYAQAYurfQCq1T+Vab/AL4dWB8nlOF0eyTZMdmBdLrOkn+jMxRaiyAapJm5WI7TMjGCQk9SnK9jsWQpd+60zgsK3JRKEJGBz8d1912ozbI10e2fIKitPgRhKkEC5mGEn6hXibWVkXXW2wXVLzmCX7pfjile55qaZr7GmgA5yAMnBlykaf/84JVt1d7V22zpQW5FDfbaOYDq8pLc2qbd0VUVDbVNJU1HSnObcsu7SzZYFKIh98ivG23FN5rFhrrnxXB8IsVqIzZbzH62JawJNWRHMxPjSLHQbNwRd0+MpKfI0tryA08J4qPmmHHL4RCdTFhEZNQHJVj/KtHtSxgW+VFY2wmBNKvOi0rmUpRcmKxnsYIlqRtBu6ACZz+4KRTFlgXUk+oGIZ3YUw8ULmKCAvyKTSK4ZbwyqmP9+b4rNdss8+JY0wPQLhACPl/A311S3tDQdCv0ufJyIA2Hpr15oTG3tpcgMHIwUJwrpQmrl+Mw+lHP1cBJUy2XGjpqKmBzqpQg+VjqZgf7lVE2xWLheRlekwNTKxe1bWBsArY/FsT72C9VRlsdsqqXb5L4JZiEwrr1fGwBN9Pvop6Lnrzol/94tJRz6vP3X15d/Orqaa8CWXgkZ/oYRfHEnJdVj4TyBTO/7sEDcvVul+DWoQFI4hY9MBVQKQ/cBy0EW3pQfr5G8t2sWetmPV3RWHbkTcn06YdzS5qmTXs7+t0LJT2NhM6WwTYPRIkbeGEiZnoDpRBGN1oI0HHZUU7jSYFTgjctjEMdjl4HxZW4F67Vtj8eEhnEBNKyWv1AM0P4y6yn9dHK/DB/Vp4IZe3iYyfoBA6ZaPbxxyoT/DG+RLLl/dWnbkmBj3JLofiaiH5S4fs9sVtxiI9JpYe67UTVLc53Tz7qNJ58UnEUastI6kEeS4TUb91WY9M+gPMjCSJyL+Q2llaUlEZI3jkneaes5FxoeXlETmm/DQaOd0bwPWTH9UaW2QDcto55MCyPMhNLG0MXoD6Uwcux60M4EABKzaKomI24qW4Nxow7kHmt4ROCmcmJ1r8JScCIeSalSEZncogpidkElobq/aWcTN0UMrJtxqTVUs4N1ddS4p/Fb/KiH1UUfy/dDdoFgWBidwWNKT767dFjYx4d4zwTxhw9NLL2AQR1+6G/wdl1gR70l4OGkgpGD4AgfDP7FcmbpecjiGOzc3PLZ782fXZ5Sd/SA+qBXL6HFytiwygXr5args2un/lOqNOfDD0RunFVaJCmuR04A9ly6zSg8AQetplChEL6eY40OHFzJZaBmP0lnMyYYCxyY+yk1YJfFMVFh4gQ1ddY9MPFxd9ju1WQB7/09BuPfvfo0a8fdoEiPKo6+rXzPHAynskn4/afH3zuSoQ1PShpKCut4RCkgORJ36yNfvvppvKyNyU/l5aMrwm90ti/9IDxznKRz9UMtRyqhVf+DHIyLnR1ZgURjbQDI4MjXQ81aQsWvIdlvYURyvNPCRaZaRGlDyLIwsRMDq9Oy/UnOJmGYEFkWutjq4nvQIVPkEqKv5ZGP6oq/gXoQfFDZF8efPfty2OcbhqefHTMGMXL/xxpPRCK2AF/ftDb0o/U7wQvL1v+ymuv/QyWqqoVy4H7eKTklXO5jU+HnsvNLanoLwcD8EBI6Q1mkRDyQKefIdjs+smzE41U+yUej+Rlyk1KvB6LTZljI7wGT9oiyYqT04hyCk/wuHgzB4vTck+QBNADMnJn66TVkonAjP9awCn+pyD0UUgBpl0g+/Lgu29PPew8EwDG3PqnbzH0GvoaCm6Mb0RjgCJELCspL7OGhvPnz5edPVsGqtjyRmPjNxHTa3LPljQ2Win+za9yG2NNKUpkMrJwJvpRGHYqc5jyMq7nkVtEuFz7+NrguSceZ2f40HK0X7eiWr5AIJ2Sn5G8gC+5iNDZldIUU9xKoAcm4De2wHYBVIvqqHRa8SMSDpCGRwigBzekfewDwIPvvhd896jzbcOTY4pffqS42Ir76Nb4RiFQhGUl1ubVBShhlobcckuBisaKipw3G2vPN5U2WpmH95t/fGWrgoVUbIZZDM35PMpUN0yZGU2X+PxsQ8H80EPfKtb9dCgkPlHU/zq1DlXLQgjYjcQnoqM0RuQiWZiyhpByEoRT+KdiswEPJBOKH5FKHwGVT7ysuhEt2V2suCGQTni4hx68DXnw7Y0QhQvkANibRw9Z60Rwa7yzJu/2n//0g5344w+HJASBkW/98Ja1T/+xU2TdOMNpM9ZmpjpiK+F/Ho65stE4/pawrKPfn1aoIIp/2R2Y1K+QmKKoqJkEIZXwQ3aJxMK4Qj5JYvEEZ47pJI+URL6/uvKYlC8gjoKKWS0hBIKjgt0qxZhoieBWUTcPoB5MBLfQ75xlwsMPj3n0P4tufW2FCG7VA7kXveejj76//yO49L/1XD66f+FH33/8LVg+ut9K6fvvvz/ARq6uAF2yNIxmRIAtEsvp4cnEJhJl7Zo2sagIcOD0aUCGonWfT+lHBAoV03jGguDI1YWtXjrAirgFfsGXJkemZiozFqWu/3rSbs53E2/tfkhRrFhX/PX0oz8VRx4D60++PH2CojcPIG5IXNAyQFH4fF3/fgS36EHL7fTuLDoI14bj3xuottkLVdNGNd3cdT5ru6peqbQ5eMwkTiauimNg/x6uaVu9GXGo+9heINTC9UAKehxJ1UOprSwxbuVQokJzp8+ilAvFtFCog12N5qRJq8nvur+kiOlV7ljtZR9MtBDh4VuHFOtUQ0eR6rRKoQA/eLS/IrhDD9R7UmRfpbkAcqMdKZRw2nC1xcw4E9QnH/I3I653GADM7Y+oep9UKtW61YnWg5/7OK4d+iQKe2w6ZyK8qKCwZrn1tg+YpuHRY5YZgJ0BAQdgcPhD58EQ9QAcA+SPvz71D5fgh5cG7xUSZ6gREWU59JoPBZOHOmpsYIhS+9Wcyve7SD1ql9hBoHjrY6vJiSrfdTds9vIV97AP4H3CTwOUtBsq1U+CofuNQ9QDNo34vLb1ubOv1zq/lP3wix2/qKZZiTQFR//oNWHrM3FkOHLrJBw7XazqRQVV0enThxbYmH3JWtOkif0AHH/wJatXT7Dd8d9tHwBM+N4VPACuKdH3neHTAy5LTonUIia/8/KnllVVjXU2v/PYqqp37t+os5GBqxMmNh2UFENxhbRol9Yng0qMuuTQ/7YDRjou+BdYcw9Jc27t3n1rdygRyhzN7+LbHTBGOsYjqX6SYoe+dSsPfsKGzgPH9ICi5xb41MNNELb+gb8+t3V7ZyLXId7h07hx45/JtjnuoAM4awN5VyQUcQ3y3RcXtMtWR2c78sft2jlEecoXjhcoOkYwNeM75hhzNFUXCxz4mq7xSKsJXjT0Ha2JAmMfTLS0DC5qFwAPJEPngYPzraPzeFOCzlBCmL/xwSrv6q21y8d15el0+LljPbx2/DMLB5to63Yyedcgp1GzPIhP+PE5hUOPRLW1c5pFh2CzoFCNmT6z2FJ/DzN1+k9/B6YB6xqP9AscM0kc+7bIirXYQw8muq5dcJse0LQ8iyRScRONxOx5IOI57z9utaTsHVfVlaSVSds92OuxllS/zHvjx1VVhT81yOkdc5uLhYmEYi1F7UyVgjOt3fV5WlPm/sSMIyou+pbD9AErLOey78SVQ+GBajqw4TGCeNmKIPRuF35yCQ1UP/W3D1S+13Z8eHfgVEsf+vErN2qav1KKxWIN0qIB8PLSaFpaEMQrIKAF0WhMALBlFFvceopSq72CBHwCW381ziRaEfFcTfUPW8eNB9UZDmOeu5J3d5/3tl9Xdb5idGF5+NbstJgA22Y5RXFPhOnhMFU6MVvA4+9FknGaGvosBd0Qiiimn5LitqzsqrEJIaqeR/NokgMJrN73ZbYCLj0cjQIkIfrVIhUcsNyPBx0Ng+vsxL7uJI+UcgRSgXRgkBgnKiooKGjXrscXTT158qS/v38gwIkTJ9aunbtq7tzCwsK9mzZtWgkQDBEPsGXLeoyHESS2Zs6KK89tu7LtyjPLIrYv76xP5mS3rFnO+N6v4RvjemX17YhyrRq3fNmDKQPYCBqlHpGbRDDfu85cSKbWizIT5mgSXZBrDRXqtbA3AuVqV3YIAPD9b4UqukhR5CsY7FD2gIS0ePSYgJBiTNI4TCoI/V7Ry07o6j9wqX1A9uMBZkc2PsBWsiPgg5QIJFIeD5OCBwGJkQI+KZFImE/JbjDJG2FUiZQnwYjKd6quVHt7X6nausySw7mHwRjeXck9X1eN7WBDJ0O6dQEw6b6PvrJdW2q5ISVfx0yoiggzMNFKv8pKv5VtLuhUpESIUt8MFQdZ1TGyHNTZoQmKrvNX8R1G2J3YUAAOHjNyWgATRFnCa6SEVBo9pr8eWLcPVMUPP/IIfGIUpGMUKnPv/HeWvq5H+9BA8b2gb38UONWlgAsDT2UA2i6SD8rBPw/usKZhrgQOpxIiGiAUIgQGJJ2y4OJMv4unYFixhBOfwuT1hXjmvvDw7Uzke3gPNwC+BOiIYrVEPcO3xnbFyPeYYwtus/2ZhQF5ppes1xaLG/PPuIRkqBgio3lXqBRyXxp9ydSpIYwuqNVd3dswjxdumeQPzvXH9fHxQZqbmzvTfiFISzoAXBVSe9fm70EoWnnyn75M264o+nZCR/CRCvbZfh06yKHsOUEEn49Z6t7CBhhUAQ7qrW/7twsTreuBovhGZWiXevTolWQ6qFVwGDR89xCwBhS9LNAbgr4NA0ZKmbivAQGqHdgHGibxGbALUgC0AAgC7lpw7MAarlQqaTG8IYhe69Xso2HFzeTwedJgmSF5BZwHA1Ch5spT3lVdFd4Ly6s6Z0qxGg/dC4ALz0wM2BBr/RRniWjsbmZ8ipklwtEwzHKUBdLKsI4ZUuAdrjKmDEzSBwOk4UcysQwGP8NVaAEZYSJxNhv5pLmZsrBlg2ZOG4HNk+sNyBzoNzIn4DEVPOLrmFHhCt/pqfyBD2VPEJaTEgZgd6SME8DxSL3RpQfW7APVDTKaGdTODHy3MAJYGx0vOwgxUcrp64bcOPRIH3z9yJqVq9aeGHg5EYpVbjQxthmqFHde4RFbjiV8KYaUoJmhwXDaY+atlJNA7Pz8Y8yiNDgvDhPG4h3xzPjlZxut4PXGxtrXmZu1T/uUrT17duv9t43W8/aZuO2CsPbdlcGxenlAPHOsMSiDW5jKhpP9qtXQjlVTPj4aQGRlXgFY8rpinmFonFiWmKFPY+LdNm7cubEj4u1MWFvYGoyInJpUV5/1neWEO/oLo8JFr46B1fBt1t3WGW1n7Lu1hb3fWT8Tob8gxXg5E4v7osM+mGjVPlD8xIkeU/zoje8Vv9yYyMQ+/FT83Y3vx4D24MZDDxcXf3vjIcXDE4joG9/2sjlUNyz/uvtWrPL1/V+jATfSgP02l7SL/MokJQIPVh6oYXAM1TAgjGs5nlzwDF9SUGvhzXKc9an8U1PbRQiNaizzH1RfgZm6ntr61DMuwNan0namWc3rW/dVnaAtIwgjeHvFCacgDaQwlI08NQdWa0ZGolwoFuP5IlFCQkL25nmb502ePDkqasGCBUELgCUcBEOgFwFDOPDE2lWFhZtWTgmeH78la/361MhIv5xTIaFSgi/1y9RkH2OO7LGvH3r1oYdefWS3lDn33gyS2d+JLWt9TNXhMOQwOWVzHuk8pa3pgdX+gxsE0IMJgt05HAHxMtiQv/tlAuNPUxQ/XIlNAC2ClKN4BJig2CN9Nuvnd/gqiv4XcXn/AUVT2gU8/3xKDSPgWx6oqu7I73wlYll5aVNuad/lfMORh2Fenlmzzl94+umzh5vOls86XHYkFybzBLfS8h7PTU1NublHckIYc6QvmPcEp0KgbULOnCrtmtwLk66OnAZslhBoyUCTBlg2wL7hEEQPpeYxao0xFlBlaMipmX7TUlPXZ2XFxwcHr9y0ae7ctYWElPC79LzRFOl7urjoO4ukAkwoVu04/Z8XC2wMirCK2I5+ZdBcC6QY51WrYUsD2QeQB6FjiicQxOpju6XE9woFsDNfPhYqfbXoyUoJ5AHGUXx/jMc59n3vlsEKD4alPxHwAN0Vm2yJ8UG1DwC3sTu/s5VwpoaSxnMSPmgnSc6Vw7c4uQ0NpQ9IXikthQHOloGpuQ29hzkfvvXy5yttYcqUz/fyCAFv5cJuQ0hCxsfPDw6esnLlpr2FhaBGT5wIDPT3Pzl10eNBQQuioiZvzsxMECHyujtnws7MgLgz4w6D2Dutra1JoHnISBQnTSGnrYkziHVUwQ2VqogZjASFFY4iUPju3uzjiHPaFe88k5ACLShSWQt37NSDiVavLwA9UBRPkBKgIYgWfF2sICofVqh8ZxL/+XA071XAAz5njGoiUdn3srYzPHC0P1Fp7BhAiLZ0tguWeKYyK/FMtRfOYVW1TQ1nS0rLy49VVs1+B6vKLemkTEdwa894yCO/fgUnsdFoRH3umoA5hnRW8hQiJEjTnMDhdfKAjE6gdUql11fQLAyA5eATgmvyaWAUMF2OOqOXl4+PUpuSwmarKRzmb1DS8BPgMrR4bfCScTPmZmVycZlYSFHyqUdP9zDRASl+8WvBUbYDnZcd1xeKjhG3+ly77MWDt3u0C/31gGkXQkG5SskhhYLIgSbiaul3j1YKgB78k+EBWdn3S92lBxBmIW25Bosi9299ruZKV57vEuvzH/AiDjeWlII2ovzItOnnsAkV5bnnuzI7907pCphw4deNASiKqBEU8UER1KfjGSx56uSsuE/8/OMyxLLbWV084KWKoGZTBrWPGniGYAsaJvQGtu0nwCtUIlqLqatjwp9hsDRiyQsN9wB6RfBdkzohLl9m2iCSK0U0PnX3tz3qTvHIrXb4Awb7Oyo6+5Vf/qXDJLBGhTFFR7suM9nSA0lIcfG6aN6hYgWWA//KdOK7h6MJ2C4IOGN8J3I4fQequ1MPusH9OHsr4AAMa2PiG61Mf1B+voZX+8033xw+nNvYUHpOkvMuzNaXW1EBQ+G7iNMjhOGwzXhn04Y6fmtChlIPO4I/kvIwAcn01C10QX4uZhxBR7+DMr/A7+uHVZauGt//XL+pHXGw89q+eXEU07633Z8I9UA1gQQ8GBMtOFSkwKK/BX9nGmfMw0APFMXHCM6Y4u/I6L7XLdypBz1gogARvLd5d+X57oMyYAHWYDk503Jy3gWUaGx6M+fp8w3lJRWHZ0FTsStrYxcfBopjoTZ4CdrMOJxYV6iO8ydJKY/HJzlr4pwOa+pzpuehpqBbfl+PUam+/Xralmx2jMHBC1rdPBhgQNIYVXHOhG87xyNZ1wMytEsPBEcVxY9gbxevO8UDayGCSqAHwD5QjJx90AOmdPqjtyJqrnTOf9B3lhNwq6iRHD369tGj7zQ25JYtDyUeOFJSeuHcNGL3cmAx9GwQGgaLd2Z5JRJ3KRnT96Pmpi1IlZKENHJXotzVw9O85Li8LiUwWBofHJieJpfprMe+20YHDwYalcbMhzIm9Iat8QcW+6BbDwScaKkAIx5WqY7Bjn1CwFkHNhcQfULeR0gPCihR/gM/VFd3xjv3neWEyfMtea68bHxFWfn58iO3co5Jqs83Xsh55+z0d5tKeiZ+7QxwHCC/My66ozSJYCZPBLTi+bL6RYvqZTK9S2jQ80u0RhabxpGT2AKRVoaLbhusx77bRqce2ByT1sGD4v+cdsNiKPbVg6JXSWAIPsKvVAAeYMeAv5Dz3VGi8usiFYyele6eIK0E2z9SSfQJaxwhPYCSmr7ih+orNc8x8c79BaG8EdiJMKH3+KbG8jfJc7U5q58uq32g9vDP08+XdZEm1554Z5qLAzXoTsFEUSicVNOJPz8AvNjJoXy/FPmQMj7ZZx8wcvHQqx3tQm/xUKhUXf4K07EM/AXmspNlpi3wZOlt9u3zpSOkBxDG/Oyt3jUwzjW3obfxDw2EsqYa/s/e2yKqvcvLD5ERpQ1N06aXlJyfTUwvBT5Epx50y8EAPPBRsmUaM9VjYDNq39yoQ4F4wzyCR+4dWipAO+dPVMB6nfbTt9auM3VTwDK9DuMvQBZABjAXwBQdtsEo8BcYFIjS73/Au9p7GZwPpUf+9o5TPPecBCN4GCl98zXJm0dKSsqrSdAifHPu3ekXyrrmxbEr3tmQtyG0lY7p/rc9EjC4HLq0YAGfj6UM6Rfsn0dTpVLlTGB40PfM7glQ+9ipga0NF/DA+fhG7f1bayLuO19e0h+5Z7+ZtQ5gzLrZ386afbYMvLVuVu7Tr5fMmlZtrXzJ2X8E2Ahl0NBxZKwX24HJsp2ALk6Siq3GXtYO5SRxZD5VhSrkRv92oR8TVL4DWxsu4IHz8Y3onOxnIpaVl1X0R1lZQ21ZWS2MfweNhKXf4Ow5zvKzFTlVTaVWNpj960abQ4ObyTD5cDUEfSDPOrGL157tlyEaQpCEAzyAAj/t1W8n/DRgLSu6BkoOIw9cEO9snJP91LZvmg5bQ25TU9ORJubhSNPhs4fP5154+lbOrZy3Z5+1Vnz2P776xPqPyGV7pG0i3dDTazoC3JTmz2lH92SahzDNgmPzK6tUoRMGGZ/YYRAMLw9cEO+ckvhJ9lt//AEsfW82cCznnftsfPpMWor1HzGJRGGJMTpX9B7agRhq7qn2/ACRA93JXXCQB8WKnEd+sqOSh5kHLtADAGWCI/g4ITvb+ieZCT42/rSMiqF1YtruOEOnQNHilZF1uqH9mMM8UOQcfdkluPVL359xqx5AUGpUzVbHoDExdEwMagQrRhRUHM2mUbYafMKMY0FjwBoqVuIoyqbZRnX/Rce2eQKKUDdpAYBanLdlvVk8tJyuDs67r2IcQldAUdTvi9ytB3BcinhAwEJd62LaRimDzYRtXrI4XitFuT54xRooKqVyVbNsaCm/hpB/wTU86BMi4RgPXKQHHQmRnIVtf8ALzqOplA/LrAf9EGOuJ4O8aEdGIXXDM/NwuEgPBj86OaQEOgAAD4tJREFUGBHmzPZelJwMQx3t6x8i2LI4SQrCHRrpPJMHLtKDQdFGEDOc2d5LmMgvcJeFgGsXpNZT6n8nHrhLD/Kv3nUkeNwKqDaZWugefwGhV62NG+qEbJ7JA3fpQd3URbHObK+kRCa2SD60JttRpOvXZyJDdVI9kwfu0oM7ArLNqS+g5BqKJTa76O8MDGGCdCg9SBZ4Jg/cpQc7t8TvdOoLKDQwSe18dLM9QOTzZzow4UEfeCYP3KUHLdnZSc5cLDRQPpJYG/lRXAsUb47CggxD3t4zeeAuPThDEP1nqHUAATI9GSYUDn8/Up6pIPlWavvQhcczeeAuPWgPjXbKTjTTyViKmBp+PRAKkQRsATJ0Q8QzeeAuPchrPeOU30hRhlicO/x2olqE69enbkCH7ph4Jg/cpQcbV82Nc4ZyOpGQNptFw+83bohbheXjQxmA0gHP5IGb9EB9h0+eceYLAvKEBSKReNj7lXFxFBHo1Dd4Jg/cpgd7995xZnsf2hzcyh3u8QeUmBU3LXVo41M74Zk8cJd9cPvjtDnObI/SG8i7KD3I7LtOQ4x+XukcDTyUB27rT5RKZzhzfE10HRnGEg43D/Zc9EvLG3rfAYRn8sBNeqBMSk1tdeYLNPQeXizFHka/MZHm4nsWXZyj0TvX+HgmD9ylB6b6jHxnKOelNPeJZ3I5UEPBJYlGr3Wyk8IzeeAu+6A9ON6p60yUnIqhzMN7eSFtIbGpgBI6OdjFM3ngNvtAQrY5M1WB2KAXU4nUMPoLqNwfy8owxHCdDKX3TB64Sw82+vu3O7M9S5PoJzPQzplwNmFAxVR7FrH2jNb5H/BMHrhLD/RpG21EKtkHymQmwnBdnav+T0+gNFUnjwoNvUTlDynSvTc8kwdu0gO0TUo6NT6R2mOWfMgdruvOBS2BmN/G5jxXJAf1TB64Sw/qg4OdSpYgu53sl4Gjrk7yjtMsoyYl6eSp9TCpt0uiqT2TB+6yD9ANn9g6l+07B0XslHy93NVxrhStTKdE68ktzV6u+krP5IG79AA08LZ8f7uC2Q1yeg/u+glQhCyvuJXExQVpujxXfaVn8sAJPRA6Bhq39Yk9gkBRVJyaQl2dtDEme2o0sWqnFhcFuOorPZMHDukBChbhHhYF5BShkVeeYvCPp5zEP36OC6A1g5yPFCsZa+PqXDEuDUFwlKJQXGZEdi6I5sWLbg8tkNEGPJMHDukBcLDkOF1g4QH3tb97l9fWlpXUWpvhxgpslct9636d6Hb6wD9tMunJNkpvoJ2fFgm1pI1H09L8/bBNmRk+apfOtuOZPHDQPkAXFXjd1iIozN+4IuIpmGUnfFy4cxhX9VQ2pRlkxNltcwYWJqS8XDQ5Fstcl3kiFNuSrc/H1dQ9Hjicz7W1MnpTihyGfrU8EFHz1rKxVeOcRfj2cT+sYA0yetEgEk9pM+sSXWEpyuPy12RJpq3K/kqJ6IRAHVxqfXocD+pOEZxWyqhzCKxNPIw4MUdNi+pWRFR7P1UdXsVkX3MCVWPHhf/xNVN6TPoANgIQ8wJl8pzCQPlQT15UCKfroMTmhMezBJysglY9Lh4OX8njeJA2k+RkUDo6xhE0p2MYQVTOa6dgfqaabc+Ehy+vgjkZh77AFM/b//ggq8CWZwlnzJRR5ubWlZWh+UMcsSyk1D4aXN7SfmI9h9zy+OZErgHXD0t4lMfxYM9MjLN21VzHsGpVKsHHMN7qXe2WfG3PbBu/vCp8sHN+YD2oqorY/scVAbZMFaDbqCE5oxCTkIFa9RC7Jbn6Vs2uValYyHz/+sR6CsfxYZqJ0+N4EDCTJCUYjyQx+xeCT/D5BI+PEZgf0IPqmit//XVZVVW4k3oAW5ZlK/Js+PAo8FMSoi6SfMGpjT7GobULlCgwlSBDN81LNJpYXhoNxWbTLvA9rGB08aD49+l52oH/sPwiqFa781N2AEgBIeHxCGmkP5O3b5t39VbvcTBToxN6wFAh/IfsNNys6ZxXF5GzuVpKKKLltDDuRCQm5Ql4JBarNiut740ljyOsWpSCqeVMLErohRcUpJ2ZsTkwODIUiw72z67LGP4JWes/sGOuQ7fxQPV7RDcID2JaW8PuOoq2u1k8TIr5TT2jWfFXbyY5k/dTy8aNDR/rnPMINt/+1Ef59IbOKwhakVBPUew8lJJnxBOEABPwCSl/dUadtS5FNZOMh4aGoMkYEwNWUFndVzvxzQseD/bDyNDI4MD6M3Eag9DeqxlOoP76QLMld8B97cLgemAWy7kwt216vgOPPkF80JKs9GLdRrryeFY/81y4E3rQKQsR25/KFos6bYREqi543hy9QWTEP5cyifqAgRpKRNls7GCmTgrR6JEWfXJz3M6owqyLHKBe61dNzhSxkAy2mMoTQ9UYZiKg9R+MHh4UKVTXClj2dMIqmbudj2Ix3RYdHZyux2H/wQrAA5inC9x//uGHt36wA398a+CCf3/ttlFtAUqx6jB+6q44YV1n9moeyfGn6ozwkoQcIC2tJY3JypqUZsqcHPW4f2DhlKzUEAEf40f7rZ/in52UlBRXB2MShq/W+0MX88LpwSvITTw47Vt0Xcty+SVlihIWBqeJcD1zfaErb1/1lef+mFteUtsP5SXltWW548tLXi+vKHm9pKQEzrgO3rMK+GHDr7F75F1IXAMsgosL63qkL5+WkZC5efPkqAULgoIePxk4d2X8+pmhfEJCSiTAdOGA6l/1eGZ+vl5024yqYRNjK63HsEH3/HHF4Iai23hw+oU64zAMLUAzM0xCVCQEPEix2IkWE2FbY3ljhZWl6u1bAEffHnfh3Xdml56vff3WzxeslWOWisYjoRhMPQ8h4PH4UgyYIpw1XTTg8YHH0pG+HNqs0TMjV29ZuXftiTUnHw/anNAed2fnxhaZEtXTtAxFYbI+N03Q3gPU8cxRpAfrdmTKhzjv2wBA4QAeg0xNCzv0AJoHV7aBh2WvN5SdhSjrcT979nwNMQ1ietXsCZIV35Q37p5eUfF671Ldpc+ebXrmwxmdCJvx4RoBn7O+IKUrmz3GP/VhG8zIOuNMWFjrnTOtdxIz6rhyPQJtA1zj42UAjkJeQZ6ZUhvYgAIUhbhpmE03Wt7/2I7pUd3EA/hLYcbhDAJDaa+/b7ckd+7I21fWDw0l52sEVeXlTU2zG8qPvBs6dvabkgio/g39y4LSZUw+ls7GXGs0xvJ48f+HpuowqUBqSeZMzpdTNEz02fknuuB2/bcByueaaocdtTP8BICAjDTqXDbUyirQj7ZeqWHcBSa/c//8jQ255RU1goiKCxXl4xtyay/k7K6RrJhd0lDar2AXDv/6VWeEglbEDo6fF5enk3MxgrQk9iaxBKF7psoaOpBrqtHTLgAmnD4+pKwyDuxwwf1bvbcxjiOjB1byN5afr8GqmsqO5B4pzS1rPMfPeftIbinM5mUl9ytz65HPVUiLRHV6UQyNNm/CSL6FB/MDhjErk2sgmjSKeAD14PqGYe47M1EfbX3O4jE8ZyN/Y2kNNh3iWHljSdmFd3JKy8pza60JQZcetHQLPEUZKFQoxBMz5mOMiSDNEuOUm6ZYHirQ5zP7zXpuBe6zD4r+e7jbTK8CU+zWbYy70Jm/0Uo+14mzZj086+na8vOzVwhybuXmlpSWVFRU5DZUlAysBz0hSnp89Sk//xTcPdPqDh0mcf1jO06Pnn4kiOOmYbaVhSK26X6gCKBtiLCavxHmcxVENJWVNeWebyypDX33NemKC+Xnn549a9bhkpIyq3pgLW+fVskyyOvrdbpRbhrQNNtLeE0xuvSg+ONJacO81yxNwYaFWxlBuI/J39gvn2tFDRkBzvuS8vLaI9OnNV04hkWUzc45hlW+dj7XmpVgnQdy0BSYAgKe5zobijzskLGPH19nT1YNN/Kg+Pc2Ul65CgF5RiOrIPuZzvzO/QDtRMGVknHjx28vOfIO50rt2fKc1SUXcqYfOZZzocSanWArv7Pcy2hU03ItMrRUOW5DXt51X0XxusHrxp08eP44rUaH+QTSJuqB11B9ZRkcd9z37M49f/6cBHb8SaVv/sx5pwkQ45zk2Ozp58rCpx3Jze1nTTTkHv51w/D+3+GGfpJ9deNGHqhUH6Rph3u+QVQjDvhla82V+8qtyEFDWe0RSz7XWU+XzpoN/Inc0llPH5l+rsk7p6mEaUj6bHD416+G+Q8PL/Lez7evctzIgyLFCy/RPsNtWhmUeaaPtlbdV95fDhoaSstymSsHr599vfQbaEjWlpSUnp++rfw5wANr6aC/+XWDqyNZ3Qr8mh2+AoQbeeCrGnMdTx/mw2rOMxlMBR89s8yStbUvzoN3L1y4UPrNhcOlpblHcsGr2RemnzsSPq2p9IKV8rM9vF04nm9nDj532geqoo/fL4gZ9n33wWWGxP9wAA923K1gxX+0jJYLBQ5DbGAXTLI3FaM7eVCs8r1uEg1/P6xclBKTtmdnWov1peeLtLSAPRs3bEjeELDnqz39i27cs5MelhlP3AGWjHpJoRp97QIchKC45tTkE/aBovWyT0zpRuuI6f1SRwt1bBmtE+p0/cuqlRr9qO8rsgXUUPD7daORB8BUVG0+Lqe/8mjTyyMAyeslfP9RO1ngbh4Adl4T6Yw2Rnrfg8sgpGhUfHySHRcaO+BmPVCcVl3biLgn2d2/NVC1mnXd13eU6gHM/Fz/UoDLpny5BxugaK7PtR3FdtPA3TwAUG1+v7n+pYJ7NsJwgaum6YL8l67v8LXXaSweCR74qo6/zxYLPdYMH/WgKJQyH7+uAGIwmvXgtGLd8Q8yClw2DdQ99IWFBqrTquLRaidCrFOpFJnX79kIwweUFn2gUAHrYFS3C8XQWsy/lsFNcemsYPcAEWAS5+EFpmvANrDfV4AYER4Uny569PrxYc1h8W8KrwIvL+H7H/j6quwZlNgDI8MD0HIpXngpb/hj///dIPPJe+l6pmpM0WkHbESIEdIDFbASXvhgpI/abw955heuP3naV1Xs62u/jQgxMjxgoPLdce0FYUoB22RgUW4P+/uNgcrTmah00+/zi+23DXti5HhQBBwb37xJ11/Ss/KGP2v6bx06r+bW49ffBzJrx2RIVjByPIC6BZiwY9IHmS/F6Ic39PG3D9Pz718/rvBV+doTrGAFI9guWKBSKT6e9L//9bxeL2cLZQXqe7AfwM6W6fX61rrjH1zLfFKhGmKbADHiPIBQFcUef2HS9Q8A/useHMMH/z3phRcyr/oWnz49tBbBgpHnQRETBVukgBdJobtzD3bj9OnTKqCnO1QKeDFh6GIA8P8Bl+YmZCg+kb8AAAAASUVORK5CYII="}}},{"cell_type":"code","source":"v = 10 # vocabulary length. Grok's real vocab size is 131,072.\nseq_len = 5 # Grok's maximum sequence length is 8192\nb = 1 # we'll use a batch size of 1 for simplicity\ntokens = torch.randint(v, (b, seq_len)) # randomly generate some token indices","metadata":{"ExecuteTime":{"end_time":"2024-06-18T08:15:44.790730Z","start_time":"2024-06-18T08:15:44.785009Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"d=8 # Grok's embedding dimension is 6144\n# initializing the embedding layer \nembedding=nn.Embedding(v,d)","metadata":{"ExecuteTime":{"end_time":"2024-06-18T08:15:44.796133Z","start_time":"2024-06-18T08:15:44.792271Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# embedding the tokens indices\nx=embedding(tokens)","metadata":{"ExecuteTime":{"end_time":"2024-06-18T08:15:44.803259Z","start_time":"2024-06-18T08:15:44.798143Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Normalization\nPrior to normalizing the input, we must scale the embeddings by the square root of their dimension.","metadata":{}},{"cell_type":"code","source":"embedding_multiplier_scale = sqrt(d)\n\nx *= embedding_multiplier_scale","metadata":{"ExecuteTime":{"end_time":"2024-06-18T08:15:44.808738Z","start_time":"2024-06-18T08:15:44.805269Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## RMS Normalization\nThe RMSNorm layer normalizes the input tensor by dividing each element by the square root of the mean squared value of the tensor elements. This normalization technique stabilizes the training process and enhances model performance by mitigating issues related to internal covariate shift. ","metadata":{}},{"cell_type":"code","source":"# first we square each entry in x and then take the mean of those values across each embedding vector\nmean_squared = x.pow(2).mean(dim=-1, keepdim=True)","metadata":{"ExecuteTime":{"end_time":"2024-06-18T08:15:44.814847Z","start_time":"2024-06-18T08:15:44.809749Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Next, we multiply x by the reciprocal of the square root of the mean squared values. To ensure numerical stability, we add a very small number, 1e-5, in case any entry is zero, preventing division errors.\nx_normed=x*torch.rsqrt(mean_squared+1e-5)","metadata":{"ExecuteTime":{"end_time":"2024-06-18T08:15:44.821377Z","start_time":"2024-06-18T08:15:44.815855Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# and finally, we multiply by a learnable scale parameter\n# it is now initialized to 1s for simplicity but in training code it will be learned\nrms_scale=torch.ones(d)\nx_normed*=rms_scale","metadata":{"ExecuteTime":{"end_time":"2024-06-18T08:15:44.827014Z","start_time":"2024-06-18T08:15:44.823330Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class RMSNorm(nn.Module):\n    def __init__(self, num_features, eps=1e-5, use_scale=True):\n        \"\"\"\n        Initialize RMSNorm module.\n\n        Args:\n            num_features (int): Number of input features.\n            eps (float): Small value added to the denominator for numerical stability.\n            use_scale (bool): Whether to use learnable scale parameter.\n        \"\"\"\n        super(RMSNorm, self).__init__()\n        self.eps = eps\n        self.scale = nn.Parameter(torch.ones(num_features)) if use_scale else None\n\n    def forward(self, x):\n        \"\"\"\n        Forward pass of the RMSNorm module.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, num_features).\n\n        Returns:\n            torch.Tensor: Normalized and optionally scaled tensor.\n        \"\"\"\n        # Calculate the mean squared value for each feature\n        mean_squared = x.pow(2).mean(dim=-1, keepdim=True)\n\n        # Normalize inputs\n        x = x * torch.rsqrt(mean_squared + self.eps)\n\n        # Apply scale if it exists\n        if self.scale is not None:\n            x = x * self.scale\n\n        return x","metadata":{"ExecuteTime":{"end_time":"2024-06-18T08:15:44.834475Z","start_time":"2024-06-18T08:15:44.828022Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Multi-Query Attention (MQA)\nThe Multi-Query Attention (MQA) mechanism is a key component of the Grok architecture. It allows the model to attend to multiple queries and key-value pairs simultaneously, enabling it to capture complex relationships in the input data. In this section, we will implement the MQA mechanism using PyTorch and explain how it works.","metadata":{}},{"cell_type":"code","source":"num_q_heads=2 # Grok has 48 query heads per layer\nnum_kv_heads=1 # Grok uses 8 key and value heads per layer\nassert num_q_heads%num_kv_heads==0 # each q needs to match up to a kv \n\n# Grok attention head matrices have a size of 128\nhead_size=4","metadata":{"ExecuteTime":{"end_time":"2024-06-18T08:15:44.842967Z","start_time":"2024-06-18T08:15:44.838934Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# now we'll initialize our self-attention weight matrices\nWq = nn.Linear(d, num_q_heads * head_size, bias=False)\nWk = nn.Linear(d, num_kv_heads * head_size, bias=False)\nWv = nn.Linear(d, num_kv_heads * head_size, bias=False)\n\n# and project x_normed out to get our queries, keys and values\nXq = Wq(x_normed)\nXk = Wk(x_normed)\nXv = Wv(x_normed)\n\n# then reshape them to separate out by head\nXq = Xq.view(b, -1, num_q_heads, head_size)\nXk = Xk.view(b, -1, num_kv_heads, head_size)\nXv = Xv.view(b, -1, num_kv_heads, head_size)","metadata":{"ExecuteTime":{"end_time":"2024-06-18T08:15:44.854914Z","start_time":"2024-06-18T08:15:44.844978Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Rotary Positional Embeddings\nRotary positional embeddings constitute a pivotal element within the Grok architecture, enhancing the model's ability to incorporate positional information from input sequences, thereby improving performance on tasks reliant on sequential reasoning. These embeddings leverage trigonometric functions to dynamically \"rotate\" the rows in query and key matrices. This rotational mechanism enables the model to discern the relative distance between tokens within a sequence. In the subsequent section, we will implement rotary positional embeddings using PyTorch and elucidate their operational principles.","metadata":{}},{"cell_type":"code","source":"assert head_size%2==0\ntheta=10000# this is a hyperparameter of RoPE that manipulates the frequency of the trig functions. Grok uses 10000","metadata":{"ExecuteTime":{"end_time":"2024-06-18T08:15:44.859034Z","start_time":"2024-06-18T08:15:44.855925Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Dynamically compute frequency cis based on the input sequence length\nexponents = torch.arange(0, head_size, 2)\nfreqs = 1.0 / (theta ** (exponents.float() / head_size))\nt = torch.arange(seq_len)\nfreqs = torch.outer(t, freqs).float()\nfreqs_cis = torch.polar(torch.ones_like(freqs), freqs)\n\n# Apply rotary embeddings to our query\nXq = torch.view_as_complex(torch.stack(torch.chunk(Xq.transpose(1, 2).float(), 2, dim=-1), dim=-1))\nXq = torch.view_as_real(Xq * freqs_cis.unsqueeze(0)).type_as(Xq)  \nXq = torch.cat(torch.chunk(Xq, 2, dim=-1), dim=-2)\nXq = Xq.reshape(Xq.shape[0], Xq.shape[1], Xq.shape[2], -1).transpose(1, 2)\n\n# and then to our key\nXk = torch.view_as_complex(torch.stack(torch.chunk(Xk.transpose(1, 2).float(), 2, dim=-1), dim=-1))\nXk = torch.view_as_real(Xk * freqs_cis.unsqueeze(0)).type_as(Xk)  \nXk = torch.cat(torch.chunk(Xk, 2, dim=-1), dim=-2)\nXk = Xk.reshape(Xq.shape[0], Xk.shape[1], Xk.shape[2], -1).transpose(1, 2)\n","metadata":{"ExecuteTime":{"end_time":"2024-06-18T08:15:44.868968Z","start_time":"2024-06-18T08:15:44.860044Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Attention Calculation\nThe next step in the Multi-Query Attention mechanism is to calculate the attention logits. This involves taking the dot product of the queries and keys, scaling the logits, and applying a mask to prevent attending to future tokens. In this section, we will implement the attention calculation using PyTorch and explain how it works.","metadata":{}},{"cell_type":"code","source":"# If the number of KV heads is different from the number of query heads, adjusts keys and values to match the query heads count.\nif num_kv_heads != num_q_heads:\n  num_queries_per_kv = num_q_heads // num_kv_heads\n  Xk = torch.repeat_interleave(Xk, num_queries_per_kv, dim=2)\n  Xv = torch.repeat_interleave(Xv, num_queries_per_kv, dim=2)\n","metadata":{"ExecuteTime":{"end_time":"2024-06-18T08:15:44.875275Z","start_time":"2024-06-18T08:15:44.869981Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Transposes Q, K, and V tensors to align them for the batch matrix multiplication in attention calculation.\nXq = Xq.transpose(1, 2)\nXk = Xk.transpose(1, 2)\nXv = Xv.transpose(1, 2)","metadata":{"ExecuteTime":{"end_time":"2024-06-18T08:15:44.881286Z","start_time":"2024-06-18T08:15:44.876286Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculates attention logits by performing a batch matrix multiplication between queries and keys\nattn_logits = torch.matmul(Xq, Xk.transpose(2, 3)).type_as(Xv)\n\n# then we scale the logits. In Grok they use 0.08838834764831845 which is the reciprocal of the square root of the head dimension\nattn_logits *= sqrt(1/head_size)\nattn_logits.shape, attn_logits","metadata":{"ExecuteTime":{"end_time":"2024-06-18T08:15:44.895776Z","start_time":"2024-06-18T08:15:44.882178Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Here, we will scale and clip our attention logits. The tanh function, a non-linear activation, compresses all entries in `scores` into the range (-1, 1). Subsequently, these scaled values are further adjusted to fall within the range (-30, 30), a chosen arbitrary range. This scaling process serves to regularize and mitigate potential numerical stability issues that could affect the subsequent softmax operation.\nmax_attn_val = torch.tensor(30.0, dtype = attn_logits.dtype)\nattn_logits = max_attn_val * torch.tanh(attn_logits / max_attn_val)","metadata":{"ExecuteTime":{"end_time":"2024-06-18T08:15:44.902319Z","start_time":"2024-06-18T08:15:44.896804Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Generate a mask tensor of shape [batch_size, num_heads, seq_len, seq_len]. The lower-triangular 1's enable the softmax function to attend to preceding tokens, while 0's ensure that each token does not attend to future tokens.\nmask = torch.tril(torch.ones((seq_len, seq_len), dtype=torch.uint8)).view(1, 1, seq_len, seq_len)\n# Expand the mask to cover the batch size and number of heads\nmask = mask.expand(b, num_q_heads, -1, -1)  # The mask now has shape [b, num_heads, seq_len, seq_len]\n\n# Convert the mask to a boolean tensor\nmask = mask.to(dtype=torch.bool)\n\n# Use a very large negative number for masked positions.\n# This large number will be turned into effectively 0 probability by the softmax later\nattn_logits = torch.where(mask, attn_logits, torch.tensor(-1e30, device=attn_logits.device, dtype=attn_logits.dtype))\n\nattn_logits.shape, attn_logits","metadata":{"ExecuteTime":{"end_time":"2024-06-18T08:15:44.916106Z","start_time":"2024-06-18T08:15:44.904772Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"attn_logits = nn.Softmax(dim=-1)(attn_logits)\nattn_logits","metadata":{"ExecuteTime":{"end_time":"2024-06-18T08:15:44.925577Z","start_time":"2024-06-18T08:15:44.917117Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# then matmul by our value projection\noutput = torch.matmul(attn_logits, Xv)\noutput.shape, output","metadata":{"ExecuteTime":{"end_time":"2024-06-18T08:15:44.934338Z","start_time":"2024-06-18T08:15:44.926588Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# and reshape to put the sequence length back into place and the outputs of our heads lined up\noutput = output.transpose(1, 2).contiguous().view(b, seq_len, -1)","metadata":{"ExecuteTime":{"end_time":"2024-06-18T08:15:44.941388Z","start_time":"2024-06-18T08:15:44.936864Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# finally we can initialize and apply our output projection that mixes the information from the heads together\nWout = nn.Linear(num_q_heads * head_size, d, bias=False)\nXout = Wout(output)\nXout.shape, Xout","metadata":{"ExecuteTime":{"end_time":"2024-06-18T08:15:44.951441Z","start_time":"2024-06-18T08:15:44.943359Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## First Residual Connection","metadata":{}},{"cell_type":"code","source":"post_attn_norm = RMSNorm(d)\nx += post_attn_norm(Xout)\nx.shape, x","metadata":{"ExecuteTime":{"end_time":"2024-06-18T08:15:44.962570Z","start_time":"2024-06-18T08:15:44.953984Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# then we'll normalize the current state of our residual for use in our MoE later\npre_moe_norm = RMSNorm(d)\nx_normed = pre_moe_norm(x)","metadata":{"ExecuteTime":{"end_time":"2024-06-18T08:15:44.970345Z","start_time":"2024-06-18T08:15:44.964583Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Mixture of Experts (MoE)\nThe Mixture of Experts (MoE) mechanism is another key component of the Grok architecture. It allows the model to combine the outputs of multiple experts to generate the final output. In this section, we will implement the MoE mechanism using PyTorch and explain how it works.","metadata":{}},{"cell_type":"code","source":"tot_num_experts = 4 # Grok has 8 experts\nchosen_num_experts = 2 # Grok also uses its top 2 experts\nwidening_factor = 2 # Grok uses a widening factor of roughly 5.33\nnoise_std = 0.1 # during training we add noise to the logits of the expert router to help prevent one expert from dominating. This would not be done during inference","metadata":{"ExecuteTime":{"end_time":"2024-06-18T08:15:44.976833Z","start_time":"2024-06-18T08:15:44.972263Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Expert(nn.Module):\n    def __init__(self, input_dim, hidden_dim, output_dim):\n        \"\"\"\n        Initialize Expert module.\n\n        Args:\n            input_dim (int): Dimensionality of the input.\n            hidden_dim (int): Dimensionality of the hidden layer.\n            output_dim (int): Dimensionality of the output.\n        \"\"\"\n        super().__init__()\n        self.layer1 = nn.Linear(input_dim, hidden_dim * 2, bias=False)  # Double the output for gating\n        self.layer2 = nn.Linear(hidden_dim, output_dim, bias=False)  # Output layer remains the same\n\n    def forward(self, x):\n        \"\"\"\n        Forward pass of the Expert module.\n\n        Args:\n            x (torch.Tensor): Input tensor.\n\n        Returns:\n            torch.Tensor: Output tensor after passing through the Expert network.\n        \"\"\"\n        # Split the output of the first layer for gating\n        x, gate = self.layer1(x).chunk(2, dim=-1)\n\n        # Apply GeLU to the gate, and then multiply element-wise\n        x = F.gelu(gate) * x\n\n        # Pass through the second linear layer\n        x = self.layer2(x)\n\n        return x\n","metadata":{"ExecuteTime":{"end_time":"2024-06-18T08:15:44.986674Z","start_time":"2024-06-18T08:15:44.978847Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Instantiate a List of 4 Expert modules\nexperts = nn.ModuleList([Expert(input_dim = d, hidden_dim = d * widening_factor, output_dim = d) for _ in range(tot_num_experts)])\n\nexperts[0].layer1.weight.shape, experts[0].layer1.weight, experts[0].layer2.weight.shape, experts[0].layer2.weight","metadata":{"ExecuteTime":{"end_time":"2024-06-18T08:15:45.009232Z","start_time":"2024-06-18T08:15:44.988694Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Router\nThe Router is a key component of the Grok architecture. It is responsible for routing the input data to the appropriate expert based on the input features. In this section, we will implement the Router using PyTorch and explain how it works.","metadata":{}},{"cell_type":"code","source":"# now we define the router that chooses which experts get used, which is just a simple linear layer with output size = tot_num_experts\nrouter = nn.Linear(d, tot_num_experts, bias=False)\nrouter.weight.shape, router.weight","metadata":{"ExecuteTime":{"end_time":"2024-06-18T08:15:45.027008Z","start_time":"2024-06-18T08:15:45.018016Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_routed = router(x_normed)\nx_routed.shape, x_routed","metadata":{"ExecuteTime":{"end_time":"2024-06-18T08:15:45.037820Z","start_time":"2024-06-18T08:15:45.029021Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# if we are training then we'll also add in some gaussian noise to help encourage the model to use all experts\n# without this, the logits of one model might quickly decay to constantly outputting 0\nx_routed = x_routed + torch.randn_like(x_routed) * noise_std # the choice of noise_st=0.1 is a hyperparameter that would need to be tuned\nx_routed.shape, x_routed","metadata":{"ExecuteTime":{"end_time":"2024-06-18T08:15:45.048065Z","start_time":"2024-06-18T08:15:45.039835Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# here we softmax to get the model's probabilities denoting which experts it things we should use\nrouting_probs = F.softmax(x_routed, dim=-1)\nrouting_probs.shape, routing_probs","metadata":{"ExecuteTime":{"end_time":"2024-06-18T08:15:45.059149Z","start_time":"2024-06-18T08:15:45.049636Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"routing_probs_for_loss = routing_probs","metadata":{"ExecuteTime":{"end_time":"2024-06-18T08:15:45.065706Z","start_time":"2024-06-18T08:15:45.061162Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"expert_gate, expert_indices = torch.topk(routing_probs, k = chosen_num_experts, sorted=True)\nexpert_gate, expert_indices","metadata":{"ExecuteTime":{"end_time":"2024-06-18T08:15:45.077709Z","start_time":"2024-06-18T08:15:45.067723Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## MoE Output Calculation","metadata":{}},{"cell_type":"code","source":"# Reshape x_normed to (b*seq_len, d) for batched processing\nx_reshaped = x_normed.view(-1, d)\nx_reshaped.shape","metadata":{"ExecuteTime":{"end_time":"2024-06-18T08:15:45.087190Z","start_time":"2024-06-18T08:15:45.079723Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Apply all experts to the input\nexpert_outputs = [expert(x_reshaped) for expert in experts]\nexpert_outputs[0].shape ","metadata":{"ExecuteTime":{"end_time":"2024-06-18T08:15:45.098349Z","start_time":"2024-06-18T08:15:45.089205Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"expert_outputs_concat = torch.cat(expert_outputs, dim=0)\nexpert_outputs_concat.shape","metadata":{"ExecuteTime":{"end_time":"2024-06-18T08:15:45.108044Z","start_time":"2024-06-18T08:15:45.100362Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# then reshape for masking\nexpert_outputs_reshaped = expert_outputs_concat.view(b, seq_len, tot_num_experts, d)\nexpert_outputs_reshaped.shape","metadata":{"ExecuteTime":{"end_time":"2024-06-18T08:15:45.118247Z","start_time":"2024-06-18T08:15:45.110066Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Using router to the output of our experts","metadata":{}},{"cell_type":"code","source":"# here we turn out expert_indices into multi-hot vectors to make them compatible with our module list of experts\nmulti_hot_indices = torch.zeros(b, seq_len, tot_num_experts)\nmulti_hot_indices.scatter_(2, expert_indices, 1)\nmulti_hot_indices.shape, multi_hot_indices","metadata":{"ExecuteTime":{"end_time":"2024-06-18T08:15:45.131305Z","start_time":"2024-06-18T08:15:45.121265Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Apply the multi-hot mask (first expand dimensions for broadcasting)\nmulti_hot_expanded = multi_hot_indices.unsqueeze(-1).expand_as(expert_outputs_reshaped)\noutput_masked = expert_outputs_reshaped * multi_hot_expanded.float()\noutput_masked.shape, output_masked","metadata":{"ExecuteTime":{"end_time":"2024-06-18T08:15:45.145295Z","start_time":"2024-06-18T08:15:45.133335Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# then weight our experts' outputs by the softmax values (which we first must broadcast to the right shape)\n# this step is important because it allows gradients to backprop through the router, meaning the model can learn which experts to use\nrouting_probs = routing_probs.unsqueeze(-1).expand_as(output_masked)\nMoE_output = output_masked * routing_probs\nMoE_output.shape, MoE_output","metadata":{"ExecuteTime":{"end_time":"2024-06-18T08:15:45.158711Z","start_time":"2024-06-18T08:15:45.147309Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# and finally sum across the chosen experts\nMoE_output = MoE_output.sum(dim=2)\nMoE_output.shape, MoE_output","metadata":{"ExecuteTime":{"end_time":"2024-06-18T08:15:45.171276Z","start_time":"2024-06-18T08:15:45.159724Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Final Residual Connection","metadata":{}},{"cell_type":"code","source":"post_moe_norm = RMSNorm(d)\nx += post_moe_norm(MoE_output)","metadata":{"ExecuteTime":{"end_time":"2024-06-18T08:15:45.179473Z","start_time":"2024-06-18T08:15:45.173337Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Getting the Output","metadata":{}},{"cell_type":"code","source":"# Multiply x by the transpose of the embedding weights to get our final output logits\nlogits = x @ embedding.weight.t()\nlogits.shape, logits","metadata":{"ExecuteTime":{"end_time":"2024-06-18T08:15:45.193014Z","start_time":"2024-06-18T08:15:45.181997Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# softmax the logits\nprobs = F.softmax(logits, dim=-1)\nprobs","metadata":{"ExecuteTime":{"end_time":"2024-06-18T08:15:45.205799Z","start_time":"2024-06-18T08:15:45.195028Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"indices = torch.argmax(probs, dim=-1)\nindices","metadata":{"ExecuteTime":{"end_time":"2024-06-18T08:15:45.215354Z","start_time":"2024-06-18T08:15:45.207814Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Loss Calculation","metadata":{}},{"cell_type":"code","source":"# create some random target indices to train on\ntarget_token_indices = torch.randint(0, v, indices.shape)\n\n# initialize the loss function\nloss_fn = nn.CrossEntropyLoss()\n\n# reshape logits to be compatible and calculate loss\nCEloss = loss_fn(logits.view(1,v,seq_len), target_token_indices)\nCEloss","metadata":{"ExecuteTime":{"end_time":"2024-06-18T08:15:45.227347Z","start_time":"2024-06-18T08:15:45.217370Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The part of note here is our expert diversity loss which is designed to encourage the model to use all of its experts roughly the same amount rather than over-relying on a small number of them. If you let the model over-rely on one or two experts, especially early in training, then what'll end up happening is it'll stick with the first ones that started to show some promise, the router will learn to only route to those two, and next thing you know you've got a bunch of untrained and unutilized experts, kinda defeating the point of MoE. To combat this, we take the probabilities outputted by the router, calculate the variance between experts in each batch, and encourage that variance to go down.","metadata":{}},{"cell_type":"code","source":"# first we sum across the sequences in the batch since during inference we want every single sequence the model runs on to use all experts\nexpert_usage = routing_probs_for_loss.sum(dim=0)\nexpert_usage.shape, expert_usage","metadata":{"ExecuteTime":{"end_time":"2024-06-18T08:15:45.238831Z","start_time":"2024-06-18T08:15:45.229356Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Compute the mean so that we can later compute the variance\nusage_mean = expert_usage.mean()\nusage_mean","metadata":{"ExecuteTime":{"end_time":"2024-06-18T08:15:45.249070Z","start_time":"2024-06-18T08:15:45.239845Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# and now variance which is just squared distance from the mean\nexpert_variance = ((expert_usage - usage_mean) ** 2).mean()\nexpert_variance\n# by encouraging the model to minimize this, it'll be forced to use all experts with roughly the same frequency","metadata":{"ExecuteTime":{"end_time":"2024-06-18T08:15:45.262188Z","start_time":"2024-06-18T08:15:45.252917Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lambadada = 500 # this is a hyperparameter that would need to be tuned","metadata":{"ExecuteTime":{"end_time":"2024-06-18T08:15:45.270458Z","start_time":"2024-06-18T08:15:45.263963Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loss = CEloss + lambadada * expert_variance\nloss","metadata":{"ExecuteTime":{"end_time":"2024-06-18T08:15:45.283086Z","start_time":"2024-06-18T08:15:45.273216Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Putting it All Together","metadata":{}},{"cell_type":"code","source":"def apply_rotary_emb(x: torch.Tensor, dim: int, theta: float = 10000.0) -> torch.Tensor:\n    \"\"\"\n    Applies rotary embeddings to the input tensor.\n\n    Args:\n        x (torch.Tensor): Input tensor of shape (batch_size, seq_len, dim).\n        dim (int): Dimensionality of the input tensor.\n        theta (float): Frequency scaling factor for rotary embeddings.\n\n    Returns:\n        torch.Tensor: Output tensor after applying rotary embeddings.\n    \"\"\"\n    # Get sequence length\n    seq_len = x.size(1)\n    device = x.device\n\n    # Dynamically compute frequency cis based on the input sequence length\n    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2, device=device).float() / dim))\n    t = torch.arange(seq_len, device=device)\n    freqs = torch.outer(t, freqs).float()\n    freqs_cis = torch.polar(torch.ones_like(freqs), freqs)  # complex64\n\n    # Apply rotary embeddings to the input tensor\n    x_ = torch.view_as_complex(torch.stack(torch.chunk(x.transpose(1, 2).float(), 2, dim=-1), dim=-1))\n    x_out = torch.view_as_real(x_ * freqs_cis.unsqueeze(0)).type_as(x)  # Ensure batch dimension is handled\n    x_out = torch.cat(torch.chunk(x_out, 2, dim=-1), dim=-2)\n    x_out = x_out.reshape(x_out.shape[0], x_out.shape[1], x_out.shape[2], -1).transpose(1, 2)\n\n    return x_out\n","metadata":{"ExecuteTime":{"end_time":"2024-06-18T08:15:45.296392Z","start_time":"2024-06-18T08:15:45.286112Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MQA(nn.Module):\n    \"\"\"\n    Implements Multi-Query Attention which supports a distinct number of attention heads for queries and key-values (KV).\n    In the case where the same number of queries and key-values are used, this implementation is equivalent to regular Multi-Head Attention.\n    \"\"\"\n    def __init__(self, config):\n        \"\"\"\n        Initializes the MQA module.\n\n        Args:\n            config: Configuration object containing model hyperparameters.\n        \"\"\"\n        super().__init__()\n\n        self.num_heads = config.num_attention_heads\n        self.num_kv_heads = config.num_key_value_heads\n        assert self.num_heads % self.num_kv_heads == 0\n        self.num_queries_per_kv = self.num_heads // self.num_kv_heads\n\n        self.hidden_size = config.hidden_size\n        self.head_dim = config.head_dim\n        self.theta = config.rope_theta\n\n        self.q_size = self.num_heads * self.head_dim\n        self.kv_size = self.num_kv_heads * self.head_dim\n\n        # Projection layers\n        self.qkv_proj = nn.Linear(self.hidden_size, (self.num_heads + 2 * self.num_kv_heads) * self.head_dim, bias=False)\n        self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=False)\n\n        # Mask tensor for attention\n        self.mask = torch.tril(torch.ones((config.max_position_embeddings, config.max_position_embeddings),\n                                     dtype=torch.uint8)).view(1, 1, config.max_position_embeddings, config.max_position_embeddings).to(dtype=torch.bool)\n\n    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Forward pass of the MQA module.\n\n        Args:\n            hidden_states (torch.Tensor): Input tensor of shape (batch_size, seq_len, hidden_size).\n\n        Returns:\n            torch.Tensor: Output tensor after Multi-Query Attention mechanism.\n        \"\"\"\n        hidden_states_shape = hidden_states.shape\n        assert len(hidden_states_shape) == 3\n        batch_size, input_len, _ = hidden_states_shape\n\n        # Linear projection to retrieve q, k, v projections\n        qkv = self.qkv_proj(hidden_states)\n        xq, xk, xv = qkv.split([self.q_size, self.kv_size, self.kv_size], dim=-1)\n\n        # Reshape to separate heads and align dimensions for attention operations\n        xq = xq.view(batch_size, -1, self.num_heads, self.head_dim)\n        xk = xk.view(batch_size, -1, self.num_kv_heads, self.head_dim)\n        xv = xv.view(batch_size, -1, self.num_kv_heads, self.head_dim)\n\n        # Apply rotary positional embeddings to queries and keys\n        xq = apply_rotary_emb(xq, self.head_dim, self.theta)\n        xk = apply_rotary_emb(xk, self.head_dim, self.theta)\n\n        # Adjust keys and values if the number of KV heads differs from the number of query heads\n        if self.num_kv_heads != self.num_heads:\n            xk = torch.repeat_interleave(xk, self.num_queries_per_kv, dim=2)\n            xv = torch.repeat_interleave(xv, self.num_queries_per_kv, dim=2)\n\n        # Transpose to align for batch matrix multiplication in attention calculation\n        q = xq.transpose(1, 2)\n        k = xk.transpose(1, 2)\n        v = xv.transpose(1, 2)\n\n        # Calculate attention logits\n        logits = torch.matmul(q, k.transpose(2, 3))\n\n        # Scale logits by the square root of head dimension\n        logits *=  0.08838834764831845\n\n        # Apply scaling and clipping using tanh to stabilize softmax\n        max_attn_val = torch.tensor(30.0, dtype=logits.dtype, device=logits.device)\n        logits = max_attn_val * torch.tanh(logits / max_attn_val)\n\n        # Apply lower-triangular mask to attention logits\n        logits = torch.where(self.mask[..., :input_len, :input_len].expand_as(logits), logits, torch.tensor(-1e30, device=logits.device, dtype=logits.dtype))\n\n        # Apply softmax to obtain attention scores\n        scores = F.softmax(logits, dim=-1)\n\n        # Weighted sum of values based on attention scores\n        output = torch.matmul(scores, v)\n\n        # Reshape attention output to combine heads back into hidden dimension\n        output = output.transpose(1, 2).contiguous().view(batch_size, input_len, -1)\n\n        # Final linear projection to map back to hidden size dimension\n        output = self.o_proj(output)\n\n        return output\n","metadata":{"ExecuteTime":{"end_time":"2024-06-18T08:15:45.316499Z","start_time":"2024-06-18T08:15:45.298411Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Expert(nn.Module):\n    def __init__(self, model_dim, hidden_dim):\n        \"\"\"\n        Initialize an Expert module.\n\n        Args:\n            model_dim (int): Dimensionality of the input to the expert.\n            hidden_dim (int): Dimensionality of the hidden layer within the expert.\n        \"\"\"\n        super().__init__()\n        self.layer1 = nn.Linear(model_dim, hidden_dim * 2, bias=False)  # Double the output for gating\n        self.layer2 = nn.Linear(hidden_dim, model_dim, bias=False)  # Output layer remains the same\n\n    def forward(self, x):\n        \"\"\"\n        Forward pass of the Expert module.\n\n        Args:\n            x (torch.Tensor): Input tensor.\n\n        Returns:\n            torch.Tensor: Output tensor after passing through the Expert network.\n        \"\"\"\n        # Split the output of the first layer for gating\n        x, gate = self.layer1(x).chunk(2, dim=-1)\n\n        # Apply GeLU to the gate, and then multiply element-wise\n        x = F.gelu(gate) * x\n        x = self.layer2(x)\n\n        return x\n\n\nclass Router(nn.Module):\n    def __init__(self, input_size, tot_num_experts, noise_std: float = 0.1):\n        \"\"\"\n        Initialize a Router module.\n\n        Args:\n            input_size (int): Dimensionality of the input to the router.\n            tot_num_experts (int): Total number of experts in the mixture.\n            noise_std (float): Standard deviation of Gaussian noise added during training for exploration.\n        \"\"\"\n        super().__init__()\n        self.tot_num_experts = tot_num_experts\n        self.router_weights = nn.Linear(input_size, tot_num_experts, bias=False)\n        self.noise_std = noise_std\n\n    def forward(self, inputs, training: bool = False):\n        \"\"\"\n        Forward pass of the Router module.\n\n        Args:\n            inputs (torch.Tensor): Input tensor.\n            training (bool): Whether the model is in training mode or not.\n\n        Returns:\n            torch.Tensor: Routing probabilities over the experts.\n        \"\"\"\n        routing_logits = self.router_weights(inputs)\n        if training:\n            routing_logits = routing_logits + torch.randn_like(routing_logits) * self.noise_std\n        routing_probs = F.softmax(routing_logits, dim=-1)\n        return routing_probs\n\n\nclass MoELayer(nn.Module):\n    def __init__(self, model_dim, expert_hidden_dim, tot_num_experts, chosen_num_experts, noise_std):\n        \"\"\"\n        Initialize a Mixture of Experts (MoE) Layer module.\n\n        Args:\n            model_dim (int): Dimensionality of the input to the MoE layer.\n            expert_hidden_dim (int): Dimensionality of the hidden layer within each expert.\n            tot_num_experts (int): Total number of experts in the mixture.\n            chosen_num_experts (int): Number of experts to use for each input.\n            noise_std (float): Standard deviation of Gaussian noise added during training for exploration.\n        \"\"\"\n        super().__init__()\n        self.model_dim = model_dim\n        self.tot_num_experts = tot_num_experts\n        self.chosen_num_experts = chosen_num_experts\n        self.experts = nn.ModuleList([Expert(model_dim, expert_hidden_dim) for _ in range(tot_num_experts)])\n        self.router = Router(model_dim, tot_num_experts, noise_std)\n\n    def forward(self, inputs, training: bool = False):\n        \"\"\"\n        Forward pass of the MoE Layer module.\n\n        Args:\n            inputs (torch.Tensor): Input tensor.\n            training (bool): Whether the model is in training mode or not.\n\n        Returns:\n            torch.Tensor: MoE output tensor.\n            torch.Tensor: Routing probabilities over the experts.\n        \"\"\"\n        b, seq_len, _ = inputs.shape\n\n        # Get the output of all the experts\n        expert_outputs = [expert(inputs.view(-1, self.model_dim)) for expert in self.experts]\n        expert_outputs = torch.cat(expert_outputs, dim=0).view(b, seq_len, self.tot_num_experts, self.model_dim)\n\n        # Get the output of the router and create the expert mask\n        routing_probs = F.softmax(self.router(inputs), dim=-1)\n        with torch.no_grad():\n            expert_indices = torch.topk(routing_probs, k=self.chosen_num_experts, sorted=True).indices\n            multi_hot_indices = torch.zeros(b, seq_len, self.tot_num_experts, device=inputs.device)\n            multi_hot_indices = multi_hot_indices.scatter(2, expert_indices, 1)\n\n        # Apply the multi-hot mask (first expand dimensions for broadcasting)\n        multi_hot_expanded = multi_hot_indices.unsqueeze(-1).expand_as(expert_outputs)\n        output_masked = expert_outputs * multi_hot_expanded.float()\n\n        # Weight our experts' outputs by the softmax values (which we first must broadcast to the right shape) and sum them\n        routing_probs_expanded = routing_probs.unsqueeze(-1).expand_as(output_masked)\n        MoE_output = (output_masked * routing_probs_expanded).sum(dim=2)\n\n        return MoE_output, routing_probs  # Also output routing_probs to be used in the loss function later","metadata":{"ExecuteTime":{"end_time":"2024-06-18T08:15:45.335400Z","start_time":"2024-06-18T08:15:45.319515Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class DecoderLayer(nn.Module):\n    \"\"\"\n    A decoder layer that integrates the Attention mechanism and MoE. It includes\n    normalization steps both before and after the MQA and MoE but never actually normalizes the residual connection.\n    \"\"\"\n\n    def __init__(self, config):\n        \"\"\"\n        Initializes a DecoderLayer.\n\n        Args:\n            config (object): Configuration object containing model parameters.\n        \"\"\"\n        super().__init__()\n\n        # Multi-Query Attention (MQA) module\n        self.mqa = MQA(config)\n\n        # Mixture of Experts (MoE) layer\n        self.moe = MoELayer(\n            model_dim=config.hidden_size,\n            expert_hidden_dim=config.hidden_size * config.embedding_multiplier_scale,\n            tot_num_experts=config.tot_num_experts,\n            chosen_num_experts=config.chosen_num_experts,\n            noise_std=config.noise_std\n        )\n\n        # RMS normalization layers\n        self.pre_mqa_norm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps, use_scale=config.use_scale)\n        self.post_mqa_norm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps, use_scale=config.use_scale)\n        self.pre_moe_norm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps, use_scale=config.use_scale)\n        self.post_moe_norm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps, use_scale=config.use_scale)\n\n        # Dropout layer\n        self.drop = nn.Dropout(config.dropout)\n\n    def forward(self, x: torch.Tensor, training: bool = False) -> torch.Tensor:\n        \"\"\"\n        Forward pass of the DecoderLayer.\n\n        Args:\n            x (torch.Tensor): Input tensor.\n            training (bool): Whether the model is in training mode or not.\n\n        Returns:\n            torch.Tensor: Output tensor after processing through MQA and MoE.\n            torch.Tensor: Routing probabilities from the MoE layer (only returned during training).\n        \"\"\"\n        # Apply MQA with normalization before and after\n        if training:\n            x = x + self.drop(self.post_mqa_norm(self.mqa(self.pre_mqa_norm(x))))\n            moe_out, routing_probs = self.moe(self.pre_moe_norm(x), training)\n            x = x + self.drop(self.post_moe_norm(moe_out))\n        else:\n            x = x + self.post_mqa_norm(self.mqa(self.pre_mqa_norm(x)))\n            moe_out, routing_probs = self.moe(self.pre_moe_norm(x), training)\n            x = x + self.post_moe_norm(moe_out)\n\n        return x, routing_probs\n","metadata":{"ExecuteTime":{"end_time":"2024-06-18T08:15:45.347537Z","start_time":"2024-06-18T08:15:45.337964Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class minGrok(nn.Module):\n    \"\"\"\n    minGrok model implementation that combines Multi-Query Attention (MQA) and Mixture of Experts (MoE)\n    for language modeling tasks.\n    \"\"\"\n\n    def __init__(self, config, tokenizer):\n        \"\"\"\n        Initializes the minGrok model.\n\n        Args:\n            config (object): Configuration object containing model parameters.\n            tokenizer (object): Tokenizer object for tokenizing text inputs.\n        \"\"\"\n        super().__init__()\n        self.config = config\n\n        # Ensure hidden_size is cleanly divisible by num_attention_heads for proper splitting and combining\n        assert config.hidden_size % config.num_attention_heads == 0\n\n        self.max_seq_len = config.max_position_embeddings\n        self.head_dim = config.head_dim\n        self.vocab_size = config.vocab_size\n        self.tokenizer = tokenizer\n\n        # Embedding matrix for converting tokens to the initial residual state and logits\n        self.embedder = nn.Embedding(self.vocab_size, config.hidden_size)\n\n        # Initialize DecoderLayers based on the number specified in config\n        self.layers = nn.ModuleList([DecoderLayer(config) for _ in range(config.num_layers)])\n\n        # Final RMS normalization layer to stabilize the output\n        self.final_norm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n\n        # CrossEntropyLoss criterion for training\n        self.criterion = nn.CrossEntropyLoss()\n\n        # Weighting parameter for the MoE loss\n        self.lambadada = config.lambadada\n\n    def calc_moe_loss(self, routing_probs_list):\n        \"\"\"\n        Calculates the Mixture of Experts (MoE) loss based on routing probabilities.\n\n        Args:\n            routing_probs_list (list): List of routing probabilities from each layer.\n\n        Returns:\n            torch.Tensor: Cumulative variance-based MoE loss.\n        \"\"\"\n        # Concatenate routing probabilities along a new dimension\n        all_routing_probs = torch.cat([x.unsqueeze(0) for x in routing_probs_list], dim=0)\n\n        # Calculate expert usage across batch and sequence dimensions\n        expert_usage = all_routing_probs.sum(dim=(1, 2))\n\n        # Calculate mean and variance across experts and layers\n        usage_mean = expert_usage.mean(dim=0)\n        expert_variance = ((expert_usage - usage_mean) ** 2).mean(dim=0)\n\n        # Sum variance across experts\n        cum_var = expert_variance.sum()\n\n        return cum_var\n\n    def forward(\n        self,\n        input_token_ids: torch.Tensor,\n        target_token_ids: torch.Tensor = None,\n    ) -> torch.Tensor:\n        \"\"\"\n        Forward pass of the minGrok model.\n\n        Args:\n            input_token_ids (torch.Tensor): Input token indices of shape (batch_size, input_seq_len).\n            target_token_ids (torch.Tensor, optional): Target token indices for training. Default is None.\n\n        Returns:\n            torch.Tensor: Logits tensor of shape (batch_size, input_seq_len, vocab_size).\n            torch.Tensor: Loss value if in training mode, else None.\n        \"\"\"\n        training = False if target_token_ids is None else True\n\n        # Convert input tokens to initial residual state using the embedding matrix\n        x = self.embedder(input_token_ids) * self.config.hidden_size ** 0.5  # Grok normalizes embedding by sqrt(hidden_size)\n\n        routing_probs_list = []  # List to store routing probabilities of each layer\n        # Process input through each DecoderLayer\n        for layer in self.layers:\n            x, routing_probs = layer(x, training)\n            if training:\n                routing_probs_list.append(routing_probs)\n\n        # Apply final normalization to the output of the last DecoderLayer\n        x = self.final_norm(x)\n\n        # Get the weights of the embedding matrix for use as the output layer\n        embedder_weight = self.embedder.weight\n\n        # Calculate logits by matrix multiplication of final output and embedding weights\n        logits = torch.matmul(x, embedder_weight.t())\n\n        if training:\n            batch_size, input_len, vocab_size = logits.shape\n\n            # Flatten logits and target_token_ids for CrossEntropyLoss\n            CEloss = self.criterion(logits.view(batch_size * input_len, vocab_size),\n                                    target_token_ids.view(batch_size * input_len))\n\n            # Calculate MoE loss to encourage usage of all experts\n            MoEloss = self.calc_moe_loss(routing_probs_list)\n\n            # Combined loss with weighting parameter lambda\n            loss = CEloss + MoEloss * self.lambadada\n        else:\n            loss = None\n\n        return logits, loss\n\n    @torch.no_grad()\n    def Sampler(\n        self,\n        logits: torch.Tensor,\n        temperature: float,\n        top_p: float,\n        top_k: int,\n    ) -> torch.Tensor:\n        \"\"\"\n        Generates token predictions from logits using sampling techniques.\n\n        Args:\n            logits (torch.Tensor): Logits tensor of shape (batch_size, vocab_size).\n            temperature (float): Temperature scaling factor for softmax.\n            top_p (float): Top-p (nucleus) sampling threshold.\n            top_k (int): Top-k sampling threshold.\n\n        Returns:\n            torch.Tensor: Predicted token indices of shape (batch_size,).\n        \"\"\"\n        logits = logits[:, -1, :]  # Select logits for the last token\n\n        # Apply temperature scaling to logits\n        logits.div_(temperature)\n\n        # Calculate softmax probabilities\n        probs = torch.softmax(logits, dim=-1, dtype=torch.float)\n\n        # Sort probabilities for top-p and top-k sampling\n        probs_sort, probs_idx = torch.sort(probs, dim=-1, descending=True)\n\n        # Apply top-p sampling mask\n        probs_sum = torch.cumsum(probs_sort, dim=-1)\n        top_ps_mask = (probs_sum - probs_sort) > top_p\n        probs_sort = torch.where(top_ps_mask, 0, probs_sort)\n\n        # Apply top-k sampling mask\n        top_ks_mask = torch.arange(probs_idx.shape[-1], device=probs_idx.device)\n        top_ks_mask = top_ks_mask.expand(probs_idx.shape[0], -1) >= top_k\n        probs_sort = torch.where(top_ks_mask, 0, probs_sort)\n\n        # Re-normalize probabilities\n        probs_sort.div_(probs_sort.sum(dim=-1, keepdim=True))\n\n        # Rearrange probabilities back to original order\n        probs = torch.gather(probs_sort, dim=-1, index=torch.argsort(probs_idx, dim=-1))\n\n        # Sample from the probability distribution\n        next_token_id = torch.multinomial(probs, num_samples=1)\n\n        return next_token_id\n\n    def generate(\n        self,\n        prompt: str,\n        output_len: int = 100,\n        temperature: float = 0.95,\n        top_p: float = 1.0,\n        top_k: int = 65,\n    ) -> str:\n        \"\"\"\n        Generates text based on a prompt using the minGrok model.\n\n        Args:\n            prompt (str): Input prompt for text generation.\n            output_len (int): Length of the generated text in tokens.\n            temperature (float): Temperature scaling factor for sampling.\n            top_p (float): Top-p (nucleus) sampling threshold.\n            top_k (int): Top-k sampling threshold.\n\n        Returns:\n            str: Generated text based on the prompt.\n        \"\"\"\n        tokens = self.tokenizer.encode(prompt)  # Encode input prompt into token indices\n        tokens = torch.tensor(tokens, device=self.config.device).unsqueeze(0)  # Convert to tensor\n\n        # Check that generated output length does not exceed maximum allowed sequence length\n        assert len(tokens) + output_len <= self.config.max_position_embeddings\n\n        for _ in range(output_len):\n            logits, _ = self(tokens[:, :self.max_seq_len])  # Get logits from model\n\n            next_token = self.Sampler(\n                logits=logits, \n                temperature=temperature,\n                top_p=top_p,\n                top_k=top_k\n            )\n\n            # Append predicted token to the sequence\n            tokens = torch.cat((tokens, next_token), dim=1)\n\n        # Decode token indices to text\n        output = self.tokenizer.decode(tokens.squeeze(0).tolist())\n\n        return output\n","metadata":{"ExecuteTime":{"end_time":"2024-06-18T08:15:45.373643Z","start_time":"2024-06-18T08:15:45.349552Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open('/kaggle/input/minigrok-training/input.txt', 'r', encoding='utf-8') as f:\n    text = f.read()\n\n# the first 200 characters. It's just one continuous text document with all of the works of shakespeare back-to-back\nprint(text[:200])\n\n# here are all the unique characters that occur in this text and how many there are\nchars = sorted(list(set(text)))\nv = len(chars)\nprint(chars)\nprint(v)","metadata":{"ExecuteTime":{"end_time":"2024-06-18T08:15:45.403532Z","start_time":"2024-06-18T08:15:45.376503Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from typing import List\n\nclass CharacterTokenizer:\n    def __init__(self, chars: List[str]):\n        \"\"\"\n        Initialize the CharacterTokenizer with a list of characters.\n\n        Args:\n            chars (List[str]): List of characters to create mappings from and to.\n        \"\"\"\n        # Create a dictionary mapping characters to indices\n        self.stoi = {ch: i for i, ch in enumerate(chars)}\n        \n        # Create a dictionary mapping indices back to characters\n        self.itos = {i: ch for i, ch in enumerate(chars)}\n\n    def encode(self, s: str) -> List[int]:\n        \"\"\"\n        Convert a string into a list of character IDs.\n\n        Args:\n            s (str): Input string to encode.\n\n        Returns:\n            List[int]: List of character IDs.\n        \"\"\"\n        # Use dictionary 'stoi' to convert each character in 's' to its corresponding ID\n        return [self.stoi.get(c) for c in s]\n\n    def decode(self, t: List[int]) -> str:\n        \"\"\"\n        Convert a list of character IDs back into a string.\n\n        Args:\n            t (List[int]): List of character IDs to decode.\n\n        Returns:\n            str: Decoded string.\n        \"\"\"\n        # Use dictionary 'itos' to convert each ID in 't' back to its corresponding character\n        return ''.join([self.itos.get(i) for i in t])\n\ntokennizer = CharacterTokenizer(chars)\n# Encoding text\nencoded_text = tokennizer.encode(\"JULIET:\\nO Romeo, Romeo! wherefore art thou Romeo?\")\nprint(\"Encoded:\", encoded_text)\n\n# Decoding back\ndecoded_text = tokennizer.decode(encoded_text)\nprint(\"Decoded:\", decoded_text)","metadata":{"ExecuteTime":{"end_time":"2024-06-18T08:15:45.413170Z","start_time":"2024-06-18T08:15:45.405550Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@dataclasses.dataclass\nclass Config:\n    \"\"\"\n    Configuration class defining hyperparameters and settings for the minGrok model.\n    \"\"\"\n\n    # Vocabulary size, typically the number of unique tokens in the dataset\n    vocab_size: int = 65\n\n    # Maximum sequence length the model might encounter\n    max_position_embeddings: int = 256  # In Grok it's 8,192\n\n    # Number of layers in the model\n    num_layers: int = 4  # In Grok it's 64\n\n    # Number of attention heads used in the attention layers of the model\n    num_attention_heads: int = 4  # In Grok it's 48\n\n    # Number of key-value heads for implementing attention\n    num_key_value_heads: int = 1  # In Grok it's 8\n\n    # Hidden size of the model, also known as the embedding dimension\n    hidden_size: int = 96  # In Grok it's 6,144\n\n    # Multiplier scale for the embedding dimension in MoE layers\n    embedding_multiplier_scale: int = 2  # In Grok it's roughly 5.33\n\n    # Total number of experts in the MoE layers\n    tot_num_experts: int = 4  # In Grok it's 8\n\n    # Number of active experts per token in the MoE layers\n    chosen_num_experts: int = 2  # In Grok it's also 2\n\n    # Standard deviation of noise injected into the router during training\n    noise_std = 0.1  # Actual value for Grok has not been shared\n\n    # Weighting factor for the loss encouraging all experts to be used\n    lambadada = 10  # Actual value for Grok has not been shared\n\n    # Number of dimensions per attention head\n    head_dim: int = 24  # In Grok it's 128\n\n    # Epsilon value used by the RMS normalization layers to prevent division by zero\n    rms_norm_eps: float = 1e-5  # Promotes numerical stability\n\n    # Scaling factor that determines frequencies for rotary positional encodings\n    rope_theta = 100.0  # In Grok and many models it's typically 10,000\n\n    # Whether to use a linear layer after normalization\n    use_scale: bool = True  # Same in Grok\n\n    # Device to run the model on (CPU or CUDA)\n    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n\n    # Dropout rate to use during training\n    dropout = 0.05\n\n\nconfig = Config()","metadata":{"ExecuteTime":{"end_time":"2024-06-18T08:15:45.425698Z","start_time":"2024-06-18T08:15:45.415391Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = torch.tensor(tokennizer.encode(text), dtype=torch.long)\nn = int(0.9*len(data)) # first 90% will be our training dataset, the rest for validation\ntrain_data = data[:n]\nval_data = data[n:]","metadata":{"ExecuteTime":{"end_time":"2024-06-18T08:15:45.623847Z","start_time":"2024-06-18T08:15:45.427713Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# data loading for training which generates a small batch of data of inputs x and targets y\ndef get_batch(split, batch_size):\n    # whether we grab from our training or validation dataset\n    data = train_data if split == 'train' else val_data\n    ix = torch.randint(len(data) - config.max_position_embeddings, (batch_size,))\n    x = torch.stack([data[i:i+config.max_position_embeddings] for i in ix])\n    y = torch.stack([data[i+1:i+config.max_position_embeddings+1] for i in ix])\n    x, y = x.to(config.device), y.to(config.device)\n    return x, y","metadata":{"ExecuteTime":{"end_time":"2024-06-18T08:15:45.633315Z","start_time":"2024-06-18T08:15:45.627859Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@torch.no_grad()\ndef estimate_loss(model, batch_size, eval_iters = 10): # to periodically estimate loss during the training loop\n    out = {}\n    model.eval() # sets model to eval mode\n    for split in ['train', 'val']:\n        losses = torch.zeros(eval_iters)\n        for k in range(eval_iters):\n            X, Y = get_batch(split, batch_size)\n            logits, loss = model(X, Y)\n            losses[k] = loss.item()\n        out[split] = losses.mean()\n    model.train() # just resets to training mode\n    return out","metadata":{"ExecuteTime":{"end_time":"2024-06-18T08:15:45.644183Z","start_time":"2024-06-18T08:15:45.636134Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = minGrok(config, tokennizer).to(config.device)\n\n# print the number of parameters in the model\nprint(sum(p.numel() for p in model.parameters())/1e3, 'K parameters')\n\nprint(model)","metadata":{"ExecuteTime":{"end_time":"2024-06-18T08:15:46.622444Z","start_time":"2024-06-18T08:15:46.587422Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training the Model","metadata":{}},{"cell_type":"code","source":"# set up the optimizer\nlearning_rate = 3e-4\nweight_decay = 0.01\noptimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n\nmax_iters = 5 #I used 100000 for the full training\n\neval_interval = 2 \n\n# batch size to use\nbatch_size = 16 # I used 64 for the full training\n","metadata":{"ExecuteTime":{"end_time":"2024-06-18T08:15:49.093834Z","start_time":"2024-06-18T08:15:47.099386Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"start_time = time.time()\n\n\nfor iter in range(max_iters):\n\n    # sample a batch of data\n    xb, yb = get_batch('train', batch_size)\n\n    # train\n    logits, loss = model(xb, yb)\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()\n\n    # every once in a while evaluate the loss on train and val sets\n    if iter % eval_interval == 0 or iter == max_iters - 1:\n        current_time = time.time()\n        elapsed_time = current_time - start_time\n        losses = estimate_loss(model, batch_size)\n        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}, time elapsed: {elapsed_time:.2f} seconds\")\n","metadata":{"ExecuteTime":{"end_time":"2024-06-18T08:16:25.709877Z","start_time":"2024-06-18T08:15:49.095844Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Generating Text","metadata":{}},{"cell_type":"code","source":"input_str = \"JULIET:\\nO Romeo, Romeo! wherefore art thou \" # the classic line\nmax_useable_output_len = config.max_position_embeddings - len(input_str)\noutput = model.generate(input_str, output_len = max_useable_output_len)\nprint(output)","metadata":{"ExecuteTime":{"end_time":"2024-06-18T08:16:35.136954Z","start_time":"2024-06-18T08:16:25.712940Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will not get any output as we have not trained the model for enough iterations. The model needs to be trained for a longer duration to generate meaningful text. If you want to train the model for a longer duration, you can set the `max_iters` variable to a higher value.\n\nIf you inspect the output carefully the output mostly consists of space .","metadata":{}},{"cell_type":"markdown","source":"# Loading my own trained model\nI had trained the model for 200000 iterations and saved the model. I will load the model and generate text using the model. It is not perfect but it still captures the essence of the text and some patterns in the text.","metadata":{}},{"cell_type":"code","source":"model = minGrok(config, tokennizer)\nmodel.load_state_dict(torch.load('/kaggle/input/minigrok/pytorch/max_iter200000/1/miniGrok200000', map_location=torch.device('cpu')))","metadata":{"ExecuteTime":{"end_time":"2024-06-18T08:16:35.189859Z","start_time":"2024-06-18T08:16:35.139965Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_str = \"JULIET:\\nO Romeo, Romeo! wherefore art thou \" # the classic line\nmax_useable_output_len = config.max_position_embeddings - len(input_str)\noutput = model.generate(input_str, output_len = max_useable_output_len)\nprint(output)","metadata":{"ExecuteTime":{"end_time":"2024-06-18T08:16:45.078369Z","start_time":"2024-06-18T08:16:35.191393Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Conclusion\nWe successfully implemented the minGrok model using PyTorch. We trained the model on a small dataset and generated text using the trained model. The model generated text that captures the essence of the text and some patterns in the text. The model can be further trained on a larger dataset to generate more meaningful text.\n\nThe text generated is not good but it still captures some patterns in the text. The model needs to be trained for a longer duration on a larger dataset to generate meaningful text. But I think this much traning was enough to show the implementation of the model.","metadata":{}}]}